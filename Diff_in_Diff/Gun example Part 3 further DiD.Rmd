---
title: "Gun Example - Part 2 - initial Diff-in-Diff"
author: "Ralf Becker"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


## Intro

Here we are looking at replicating aspects of this paper:

Siegel et al (2019) The Impact of State Firearm Laws on Homicide and Suicide Deaths in the USA, 1991–2016: a Panel Study, J Gen Intern Med 34(10):2021–8.

In Part 1 - Data acquisition and management, we organised the data in a new datafile saved in "US Gun_example_v3.csv". We will continue with the work working from that datafile


## Load packages

Let's start by loading the packages we will use throughout this work.

```{r}
library(tidyverse)
library(readxl)
library(stringr)
library(ggplot2)
library(plm)          # for panel data
library(sandwich)     # for cluster robust standard errors
library(stargazer)    # for nice regression output
library(stringr)      # to extract strings
library(coefplot)     # to create coefficient plots
library(AER)          # to use lht

source("Stargazer_HC.R")
```

## Load Data


```{r, eval = FALSE}
# ensure you know where you saved this file from Part 1
data3 <- read.csv("US Gun_example_v3.csv")

# we also load the laws codebook just in case we need to refer to it
law_data_codebook <- read_excel("../data/Firearmlaws codebook_0.xlsx")
names(law_data_codebook) = make.names(names(law_data_codebook)) # This elimnates spaces in variable names
```

```{r, echo = FALSE}
data3 <- read.csv("../data/US Gun_example_v3.csv")

# we also load the laws codebook just in case we need to refer to it
law_data_codebook <- read_excel("../data/Firearmlaws codebook_0.xlsx")
names(law_data_codebook) = make.names(names(law_data_codebook)) # This elimnates spaces in variable names
```

Let's recall what variables are included in `data3`:

```{r}
names(data3)
```


## Summary Stats and Data Exploration

Let's calculate some firearm death statistics. The data from the CDC deliver an age adjusted rate which indicates the number of deaths for every 100,000 of population in state $s$ in a particular year $t$. Let's average that rate in each sate across all years.

```{r}
table2 <- data3 %>% group_by(State) %>% 
                          summarise(avg_fd.pc = mean(firearm.deaths.pc,na.rm = TRUE), n=n()) %>% 
                          arrange(-avg_fd.pc) %>%  # sort in decreasing order of avg_aarate
                          print()
```

If you are working on replicating or extending some published work you should try and see whether your data are the same as the ones used in the publication you are working off. This is best done by matching your own summary statistics to those in the paper. The Siegel et al (2019) paper does not really have any descriptive statistic, but report some data for 2016 (their Table 2). They report, for that year an age adjusted homicide rate of 14.2. Let's look at our data for Louisiana in 2016:

```{r}
data3 %>% filter(Year == 2016, State == "Louisiana") %>% 
  select(State, Year, firearm.deaths.pc, homi.pc)
```
In the table above you can see that we have a value of 21.20 (per 100,000 population) firearm deaths in Louisiana for 2016. That is not the same as the 14.2 in the paper but we wouldn't expect this as this doesn't measure homicides. But most likely it is quite highly correlated and it is certainly, size wise, in the same ballpark.

The reason for the difference is that the data we got from the F.B.I. website (`homi.pc`) are not age adjusted data (as are the data from the CDC). The firearm deaths data, while being age adjusted, does not actually record whether a death was a suicide or homicide, but only registered the mechanism of death. Of course a death by firearm could be either a homicide or a suicide. Therefore, we really do not have the data to replicate what they have actually done. 

This means that in the later analysis we can either use the age adjusted firearm deaths data (`firearm.deaths.pc`) or the non-age adjusted homicide data  (`homi.pc`). We could of course also attempt to apply the age adjustment procedure, but we will not do this here. For now we will use `firearm.deaths.pc` as the main outcome variable.


Let's look at a few time-series plots of the data. Whenever you are working with data that have a time-series dimension you should do this as this will give the reader a better understanding of the data.

```{r}
plot1data <- data3 %>% filter(State %in% c("Alabama","Tennessee","Maine", "New York"))
p1 <- ggplot(plot1data, aes( x = Year, y = firearm.deaths.pc, color = State)) +
            geom_line() +
            labs(title="Age Adjusted Death by Firearm Rates")
p1
```

There clearly seems to be an upward trend in the number of firearm deaths after 2015,but for New York in this selection of States.

Let's remind ourselves how the policy variables are coded in the data.

```{r}
unique(data3$pol_ubc)
```
You can see that the `pol_ubc` variable only contains 0s and 1s. That is the same for the other policy variables `pol_vmisd` and `pol_mayissue`. Let's calculate summary statistics for these:

```{r}

summary(data3[c("pol_ubc","pol_vmisd", "pol_mayissue")])
```

Let us also repeat a plot we already looked at in Part 1.

```{r}
ggplot(data3, aes(x = Year, y = State)) + 
  geom_raster(aes(fill=pol_ubc)) + 
  scale_fill_gradient(low="grey90", high="red") +
  labs(x="Year", y="State", title="Policy - Universal Background Check") 

```

This illustrates that some states, e.g. Maryland, had this policy in place throughout the entire sample period, others never (e.g. Indiana) and yet others introduced a universal background check policy during the sample period. such as Colorado. Without the latter type of states there would be no difference-in-difference setup.

Also create the above image for the `pol_vmisd` variable. You will see that there is no variation in that policy in our sample. That means that we cannot estimate a Diff-in-Diff model.

## Initial Diff-in-Diff

### Policy set-up

We wish to evaluate whether a particular gun control policy has a causal effect. The policy considers re the universal background checks policy (`pol_ubc`), policies that prevent person with recorded violent misdemeanors from owning a gun (`pol_vmisd`) and so called may issue laws which gives. From the laws codebook:

```{r}
print(toString(law_data_codebook[law_data_codebook$Variable.Name == "mayissue","Detailed.Description.of.Provision"]))
```

Let us look at the development of this policy through time:

```{r}
ggplot(data3, aes(x = Year, y = State)) + 
  geom_raster(aes(fill=pol_mayissue)) + 
  scale_fill_gradient(low="grey90", high="red") +
  labs(x="Year", y="State", title="Policy - May Issue Policy") 

```

As you can see, these policies are generally being faced out, meaning that states are loosing the ability to deny firearm permits. In terms of a Diff-in-Diff setup we like to evaluate plicies that are being introduced, not phased out. Therefore, in the Siegel et al. paper, you will see a reference to a "Shall issue" policy, which is basically the absence of a "May issue" policy. This is defined as `1-pol_mayissue`. So let is add that variable to the data frame.

```{r}
data3$pol_shallissue <- 1 - data3$pol_mayissue 
```

Let us look at the development of this policy through time:

```{r}
ggplot(data3, aes(x = Year, y = State)) + 
  geom_raster(aes(fill=pol_shallissue)) + 
  scale_fill_gradient(low="grey90", high="red") +
  labs(x="Year", y="State", title="Policy - Shall Issue Policy") 

```
Now you can see that this is a policy being introduced. The policy is really to remove a gun control policy (that is still a political decision).



### Estimation

We will now estimate a TWFE model where $log(fd.pc_{st})$ is the log of the age adjusted death rate by Firearm. At this stage there are no covariates.

$$log(fd.pc_{st}) = \alpha_s + \gamma_t + \tau~ pol_{st} + error_{st}$$

We extract the data we need and set them up as a panel data set. Before doing that we also calculate the log of the two potential outcome variables.

```{r}
data3 <- data3 %>% mutate(log.fd.pc = log(firearm.deaths.pc), log.homi.pc = log(homi.pc))
pdata <- pdata.frame(data3, index = c("State","Year")) # defines the panel dimensions
```

Now we estimate the model for the `pol_ubc` and `pol_mayissue`. Recall that we cannot estimate a model for `pol_vmisd` as there is no policy variation in that variable. Actually you should try and see what happens.

```{r}
mod_twfe1 <- lm(log.fd.pc~State+Year+pol_ubc, data = pdata)
se_twfe1 <- sqrt(diag(vcovCL(mod_twfe1, cluster = ~ State)))

mod_twfe2 <- lm(log.fd.pc~State+Year+pol_shallissue, data = pdata)
se_twfe2 <- sqrt(diag(vcovCL(mod_twfe2, cluster = ~ State)))

stargazer(mod_twfe1, mod_twfe2, keep = c("pol_ubc","pol_shallissue"), type="text", se=list(se_twfe1,se_twfe2), 
          digits = 6, notes="Cluster (State) Robust standard errors in parenthesis")
```

The result for `pol_shallissue` is very close to that in Siegel et al. (2019), which is 0.082. Recall that the policy is actually the removal of a gun control policy and therefore the positive sign makes sense. The coefficient for the universal background policy has halfed (from -0.173 in the paper) to -0.086 here. This coefficient is now also statistically insignificant. 

In the Siegel et al. paper they include state and year fixed effects (see their model on page 2022). Have we really included these by adding "State+Year"? In dataset `data3` the `Year` variable was an integer variable and if we include an interger or numerical variable then R will merely estimate one coefficient for that variable.

You can test what happened by looking at how many and which coefficients were estimated:

```{r}
names(coef(mod_twfe1))
```

As you can see the model did include year and state fixed effects. The reason for this is that we used the panel dataset and one of the things that happen when we create a panel dataframe (using `pdata.frame`) is that, if it uses `Year` as the time index it translates that variable into a factor (categorical) variable. It is for this reason that R did include Year fixed effects.

### Investigating the difference

One possible reason for this is that a lot of the policy variation in that policy may have happened before 2001 and hence we do not see that in our sample. If you go back to the original policy file and check you will find that three states introduced universal background check policies before 2000, Cennecticut (CT), Massachusetts (MA) and Pennsylvania (PA).

A different reason why the results may be different is that we cannot be certain about the policy coding used by Siegel et al. (2019). We combined the two policies that had explicitly "universal" in their naming, however, there are more background check policies. In Table 1 of their paper they state which states had the policy implemented in 1991. They are CA, IL, MA, NJ and RI. However, in the above image we see that only CA and RI have the policy implemented in our coding. 

You could go back to the policy details and you may find that other policies also imply background checks, such as "universalpermit" and "universalpermit". So you could see whether you can replicate Siegel et al. (2019) policy coding. I was unable to replicate this. This merel points to the importance of properly documenting what you do in your work such that someone reading your paper can replicate the work.

Here we will not investigate this line of enquiry any further.

Let us investigate why our results are different. The first possibility is that we are not yet using all the covariates that were included in the Siegel et al. (2019) paper. Once they are included, coefficients may well change as.


```{r}
mod_twfe1c <- lm(log.fd.pc~State+Year+pol_ubc+prop18.24+law.officers.pc+ur+vcrime.pc+alcc.pc,incarc.pc+log(Population), data = pdata)
se_twfe1c <- sqrt(diag(vcovCL(mod_twfe1c, cluster = ~ State)))

mod_twfe2c <- lm(log.fd.pc~State+Year+pol_shallissue+prop18.24+law.officers.pc+ur+vcrime.pc+alcc.pc,incarc.pc+log(Population), data = pdata)
se_twfe2c <- sqrt(diag(vcovCL(mod_twfe2c, cluster = ~ State)))

stargazer(mod_twfe1c, mod_twfe2c, keep = c("pol_ubc","pol_shallissue"), type="text", se=list(se_twfe1c,se_twfe2c), 
          digits = 6, notes="Cluster (State) Robust standard errors in parenthesis")
```

You can see that the inclusion of the covariates does not change the main conclusion. In fact, now the coefficient for the `pol_shallissue` variable has also moved away from that published in the paper, although it is still statistically significant.

Let us interpret the coefficient to the the `pol_shallissue` variable. The estimated coefficient is 0.0465, let's say for ase of calculation 0.05. So if the policy went from 0 to 1 (here meaning the abolition of a "may issue" policy or the introduction of a "shall issue" policy) is estimated to have an affect of increasing the log of the firearm deaths by 0.05 log points. This is equivalent to a little more than 5.1% increase (using $100(exp(0.05)-1)$). What does that mean. The average of the `firearm.deaths.pc` variable is 12 firearm deaths per year per 100,000 population. 5% of that is 0.6 per 100,000. In Texas there is a population of about 30 Million. If we project that policy impact on a population of 30,000,000 that would be the equivalent of 180 people a year. 

Recall that we recognised that our data for the outcome variable is not the same as that used in Siegel et al. (2019). Above we used `firearm.deaths.pc` but we also acquired the `homi.pc` data from the F.B.I.. Let us re-run the above models (with covariates) with that outcome variable.


```{r}
mod_twfe3c <- lm(log.homi.pc~State+Year+pol_ubc+prop18.24+law.officers.pc+ur+vcrime.pc+alcc.pc,incarc.pc+log(Population), data = pdata)
se_twfe3c <- sqrt(diag(vcovCL(mod_twfe3c, cluster = ~ State)))

mod_twfe4c <- lm(log.homi.pc~State+Year+pol_shallissue+prop18.24+law.officers.pc+ur+vcrime.pc+alcc.pc,incarc.pc+log(Population), data = pdata)
se_twfe4c <- sqrt(diag(vcovCL(mod_twfe4c, cluster = ~ State)))

stargazer(mod_twfe3c, mod_twfe4c, keep = c("pol_ubc","pol_shallissue"), type="text", se=list(se_twfe3c,se_twfe4c), 
          digits = 6, notes="Cluster (State) Robust standard errors in parenthesis")
```
As you can see, we now get coefficients that are closer to those published in Siegel et al. (2019), but the coefficients are estimated to be statistically not different from 0.


### A note of caution

In the above we applied a TWFE model on a setup with staggered policy. Recently a lot of methodological research has been undertaken that investigates the properties of TWFE type models when there is staggered policy timing (meaning that different states implement policy at different times). It turns out that the resulting effects crucially depend, not only on the parallel trends assumption but also on assuming that the policy has the same effect in different states. Above we estimated that the effect of introducing the "may issue" policy was about 0.05 log points. Basically what we need is that this effect is the same in different states.

It goes without saying that this assumption may be a valid one in some situations, but surely is not a correct assumption in some others. The current research frontier is designing adapted methods that allow for such policy heterogeneity. In this unit we are not covering such advanced methods and you are not expected to adopt these for your project. But we need you to be aware that what you learn in this unit is not the end of the road ... there is always more to learn.

## Event studies approach

One of the issues with the TWFE approach is that we assume that the policy implementation has an immediate impact on the outcome variable. However, it is not obvious that the data would support such an immediate impact. There is a way to relax this assumption and let the data decide about the timing and strength of the policy effect. 

This method is really a method to be implemented for datasets that do not have staggered policy effects. Of course staggered policy implementation is a standard feature of many datasets. Let's look again at the implementation of the "shall issue" policy.

Let us look at the development of this policy through time:

```{r}
ggplot(pdata, aes(x = Year, y = State)) + 
  geom_raster(aes(fill=pol_shallissue)) + 
  scale_fill_gradient(low="grey90", high="red") +
  labs(x="Year", y="State", title="Policy - Shall Issue Policy") 
```

From the full dataset we can create subsets of data that create a nice treatment and control group set. For instance pick out the following nine states:

```{r}
State_sel <- c("Alabama", "Illinois", "Delaware", "Connecticut", "Rhode Island", "New York", "New Jersey", "Massachusetts", "Maryland")
pdata_sub1 <- pdata %>% filter(State %in% State_sel)
```

Now replicate the above plot with this subset.

```{r, fig.height = 2, fig.width = 6}
ggplot(pdata_sub1, aes(x = Year, y = State)) + 
  geom_raster(aes(fill=pol_shallissue)) + 
  scale_fill_gradient(low="grey90", high="red") +
  labs(x="Year", y="State", title="Policy - Shall Issue Policy") +
  guides(x = guide_axis(n.dodge = 2)) # this offsets every 2nd x axis label
```

Here you can see that we are having two states in the treatment group (Illinois and Alabama) who both implement the policy from 2013 onward and seven states in the control group (all others). If you have such a "clean" setup, an event study approach is straightforward to implement. We need to create new variables which interact a dummy variable which takes the value 1 for those states that are ever treated (here Illinois and Alabama) with the Year variable.

```{r}
State_treat <- c("Alabama", "Illinois")
# this creates the ever treated dummy variable
pdata_sub1 <- pdata_sub1 %>% mutate(d = case_when(State %in% State_treat ~ 1,
                                                  TRUE ~ 0))
mod_event_sub1 <- lm(log.fd.pc~State+d*Year, data = pdata_sub1)
stargazer_HC(mod_event_sub1, type_out="text")
```

Note that we did not use cluster robust standard errors. Convention is that you should only use them if you have at least 50 clusters, here we only have 4. We therefore use the standard robust standard errors.

Note what variables were included by specifying the model as "log.fd.pc~State+d*Year":

* a constant (which is included by default)
* State dummy variables, 8 of them, Rhode Island is the base state here

The remaining variables are included as a result of "d*Year"

* The dummy variable d, the ever treated variable, but note that this is colinear as it is identical to "StateIllinois + StateAlabama". R realises this and does not report a coefficient, it excludes it from the model
* Year dummy variables, for every year but the base year 2001m they are labeled "Year2002" to "Year2021"
* Interaction terms between d and the Year dummy variables, they are labeled "d:Year2002" to "d:Year2021".

All of these are automatically included by adding "d*Year" to the regression specification. 

It is these last set of coefficients, the interaction terms, that give us the year specific policy effects. Let us display them graphically. 

```{r}
# this line selects all the coefficient names that contain "d:Year"
coef_keep = names(coef(mod_event_sub1))[grepl("d:Year",names(coef(mod_event_sub1)))]

# this creates a coefficient plot
coefplot(mod_event_sub1, coefficients = coef_keep, innerCI = 0, horizontal = TRUE) +
  guides(x = guide_axis(n.dodge = 2)) # this offsets every 2nd x axis label
```

The confidence bars indicate +/- 2 standard errors. There are two important observations from this. First, all the coefficients up to the period of policy implementation (2015) are close to 0 and certainly statistically insignificant. This suggests that there is no evidence against the parallel trends assumption prior to the policy implementation. Second, the coefficients after 2015 indicate that in the policy states there was a somewhat positive effect of the policy, although the individual coefficients are all not statistically significantly different from 0. The confidence intervals still include the 0.

We could run a hypothesis test to test the hypothesis that all of the post policy coefficients are equal to 0.

```{r}
coef_test <- c("d:Year2016","d:Year2017","d:Year2018","d:Year2019","d:Year2020","d:Year2021")
lht(mod_event_sub1,coef_test, white.adjust = TRUE)
```

When you do this you will get an error message most likely saying something like "there are aliased coefficients in the model". The issue here is that the model still includes the variable `d`, the ever treated dummy. R excluded it in the regression output but the `lht` procedure does not do that and the above is R's charming way of saying that something isn't right in the model (`lm` brushed this underneath the carpet).

So what we really need to do is to speify the same model as above without the `d`. But that means that we cannot use the neat trick of including `d*Year`.

```{r}
X <- model.matrix(mod_event_sub1) # puts all the explan variables used in mod-event_sub into a matrix
X. <- X[, -c(1,10)]
mod_event_sub1b <- lm(log.fd.pc~X., data = pdata_sub1)
stargazer_HC(mod_event_sub1b, type_out="text")
```

```{r}
coef_test <- c("X.d:Year2016","X.d:Year2017","X.d:Year2018","X.d:Year2019","X.d:Year2020","X.d:Year2021")
coef_test <- c("X.d:Year2017")

lht(mod_event_sub1b,coef_test, white.adjust = TRUE)
```