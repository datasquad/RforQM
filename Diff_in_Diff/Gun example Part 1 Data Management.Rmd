---
title: "Gun Example"
author: "Ralf Becker"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


## Intro

Here we are looking at replicating aspects of this paper:

Siegel et al (2019) The Impact of State Firearm Laws on Homicide and Suicide Deaths in the USA, 1991–2016: a Panel Study, J Gen Intern Med 34(10):2021–8.

In this paper the authors evaluate whether the introduction of different types of gun control laws have a causal impact on a range of outcome variables, e.g. the rate of gun related deaths. In these walk-throughs we will attempt to first access the data used in the paper and then to replicate the empirical results. We will be unable to obtain the exact same dataset and therefore our attempt will only be partially successful. But in any case the process shown here will reflect the process of an empirical piece of research.


## Load packages

Let's start by loading the packages we will use throughout this work.

```{r}
library(tidyverse)
library(readxl)
library(stringr)
library(ggplot2)
library(plm)          # for panel data
library(sandwich)     # for cluster robust standard errors
library(stargazer)    # for nice regression output
library(stringr)      # to extract strings
```

## Data

The core data are information about Gun Laws and fatality statistics

### Gun Law data

Siegel and co-authors collated this information and published them on <https://www.statefirearmlaws.org/>. The following uploads the database ("Firearmlaws DATABASE_0.xlsx"). Each row indicates a state-year and in columns indicates whether in a particular state-year a policy is in place. From the database you can also download a spreedsheet that adds explanations to the varable definitions. This is important information and we download it also ("Firearmlaws codebook_0.xlsx").

```{r}
# set data directory
datadir <- "../data/"

law_data <- read_excel(paste0(datadir,"Firearmlaws DATABASE_0.xlsx"))
law_data_codebook <- read_excel(paste0(datadir,"Firearmlaws codebook_0.xlsx"))
names(law_data_codebook) = make.names(names(law_data_codebook)) # This elimnates spaces in variable names
```

The last line of the bove code ensures that variable names have no spaces. For instance it turns the variable name `Sub Category` into `Sub.Category`. This makes working with variables much easier.

The following lists all the different policy categories used.

```{r}
names(law_data)
```

You can get details on particular policies by looking at the codebook. For instance if you wish to know what policy is "junkgun".

```{r}
print(toString(law_data_codebook[law_data_codebook$Variable.Name == "junkgun","Detailed.Description.of.Provision"]))
```

The policies are also in a number of different categories (variable `category` in `law_data_codebook`) and sub categories (variable `Sub.category`). Let's see what categories there are and how many different policies fall under each of them.

```{r}
table(law_data_codebook$Category)
```

The most recent observation from the policy database is 2020.

### State level fatality data

Data on state level fatalities and their reason can be obtained from the [US Center for Disease Control](https://wisqars.cdc.gov/), in particular the "Fatal and Nonfatal Injury Reports". Following the lead from the Siegel et al (2019) paper we are after the age adjusted homicide rates.

Data are in state level files for Years. For instance you can find a file that provides annual information on causes of deaths and also broken down by the race of the deceased. The data for the state of Alabama is available from ("AL by race.csv"). Let's upload these data.

```{r}
deaths_data_race <- read_csv(paste0(datadir,"AL by race.csv"), na = "--")
names(deaths_data_race) = make.names(names(deaths_data_race))
# only keep rows that start with the state name 
deaths_data_race <- deaths_data_race %>% filter(State == "Alabama")
head(deaths_data_race) # displays the first few observations
```

When checking the notes to the table you will realise that they do not report number of deaths if they are below 10. For example there is no reported number of deaths by "Cut/Pierce" for "Asien / HI Native / Pac.Islander" ethnicity. That does not mean that there were none. All we know that there were not more than 10.  For values below 20 they are labeled as unsafe values (labelled with a `**`).  The fact that numbers below 10 are not recorded means that we cannot get statewide numbers by aggregating across all races. 

While it would be very interesting to do this analysis broken down by races, and the above data are suitable for such an analysis, we will not pursue this line here and we will have to base our analysis on different datasets, not the datafiles that break the numbers down by race.


Let's look at the data for Alabama not broken down by race ("AL.csv"):

```{r}

deaths_data <- read_csv(paste0(datadir,"AL.csv"), na = "--")
names(deaths_data) = make.names(names(deaths_data))
# only keep rows that start with the state name 
deaths_data <- deaths_data %>% filter(State == "Alabama")

# remove "**", needs applying twice
remove_astast <- function(x){ return (str_remove(x, "[**]"))}
test <- data.frame(lapply(deaths_data,remove_astast))
deaths_data <- data.frame(lapply(test,remove_astast))

# turn data into numeric
deaths_data$Deaths <- as.numeric(deaths_data$Deaths)
head(deaths_data,10)

```

Let's explore which types of causes of death are in the dataset.

```{r}
print("Causes of death")
unique(deaths_data$Mechanism)

```

You will see that these indicate the "mechanism" of death, but do not indicate whether a death was a homicide, accident or suicide. Recall that we are really after the homicide data. We are returning to this point a little later.

When downloading the data from CDC website you can only download the data for one state at the time. Here are the filter settings used when downloading data from the WISQARS Fatal and Nonfatal Injury Reports.

image: ![](WISQARSfilter.jpg)

After downloading these data you should have 51 such files, one for each state. This is the point where we have to do a bit of organising. Make sure that the files have a consistent and clear naming structure. Here we saved the files under the two letter abbreviation for each state, for instance "AL" for Alabama. At this stage you should make sure that you have a list of all the states and the associated abbreviations. You can get such a list from many places. Search the internet for something like "list of us states and abbreviations". 

I saved such a table into an Excel file

```{r}
USstates <- read_excel(paste0(datadir,"USStates_Dictionary.xlsx"))
head(USstates)
```

Working with 51 files is awkward and we should merge these data together into one dataframe. If we know that there will be 52 files all structured the same way (as they all come from the same source) and all named consistently, using the two letter abbreviation, then we should be able to "automate" the merging process.

Roughly the following code does the following. Import the data from the first state (Alabama) and save that into `deaths_data`. Then loop through the remaining states. Upload the new states data into `temp_data` and add (using the `bind_rows` function) `temp_data` into `deaths_data`. Then move on to the next state and do the same. In that day the `deaths_data` file will grow until it contains the data for all 51 states. 

```{r}
# start with first state
USstates_code <- USstates$State_code[1]   # pick first state code
USstates_name <- USstates$State[1]        # pick first state name
filename <- paste0(datadir, USstates_code, ".csv") # create text variable with filename to upload

deaths_data <- read_csv(filename, na = "--")
# only keep rows that start with the state name 
deaths_data <- deaths_data %>% filter(State == "Alabama")

for (i in seq(2,nrow(USstates))){
  USstates_code <- USstates$State_code[i]
  USstates_name <- USstates$State[i]
  filename <- paste0(datadir, USstates_code, ".csv")
  
  # get the next file
  temp_data <- read_csv(filename, na = "--")
  # only keep rows that start with the state name 
  temp_data <- temp_data %>% filter(State == USstates_name)
  
  # attach to deaths_data
  deaths_data <- bind_rows(deaths_data,temp_data)

}


#after loop
names(deaths_data) = make.names(names(deaths_data))

# remove "**", needs applying twice
remove_astast <- function(x){ return (str_remove(x, "[**]"))}
test <- data.frame(lapply(deaths_data,remove_astast))
deaths_data <- data.frame(lapply(test,remove_astast))

# turn data into numeric
deaths_data$Year <- as.numeric(deaths_data$Year)
deaths_data$Deaths <- as.numeric(deaths_data$Deaths)
deaths_data$Age.Adjusted.Rate <- as.numeric(deaths_data$Age.Adjusted.Rate)
deaths_data$Population <- as.numeric(deaths_data$Population)
deaths_data$Crude.Rate <- as.numeric(deaths_data$Crude.Rate)
deaths_data$Years.of.Potential.Life.Lost <- as.numeric(deaths_data$Years.of.Potential.Life.Lost)

```

After going through this code you should find that t`deaths_data` has 20,895 rows of data and 8 variables.

At this stage we have the information on the laws and the information on the death rates in two different data objects, `law_data` and `deaths_data`. Let us now merge these two data sets into one big data object. First we compare the variable names to ensure that we have matching variables

```{r}
names(deaths_data)
names(law_data)
```
When you merge data files you need to be very clear what you wish to merge. Here we have data at the State-Year level. So we want to merge data from the same State-Year together into the same row. In `law_data` we have one row of data for each state-year. 

```{r}
law_data %>% filter(state == "Alabama", year == "2015")
```

But note that in `deaths_data` we actually have 20 rows of data for each state-year. One row for each mechanism of death.

```{r}
deaths_data %>% filter(State == "Alabama", Year == "2015")
```

The aim here is to keep the 20 rows of data for the different mechanisms and add the same law data to each of the 20 rows of data (say for Alabama in 2015).

We want to match on State and Year, but note that the variables are capitalised in `deaths_data` and not capitalised in `law_data`. As R is case sensitive we will have to harmonise the names before merging.

```{r}
names(law_data)[names(law_data) == "state"] <- "State"
names(law_data)[names(law_data) == "year"] <- "Year"
merge_data <- merge(deaths_data,law_data, all.x = TRUE)
```

We end up with 20895 rows of data. This is the same number of row as in `deaths_data`. Check out what would have happened had you set the `all.x` option to FALSE. Check out the help for the `merge` function to understand what this option does.

It is important to understand why you have the number of observations you have. Here we have 51 states, 21 years and 21 methods of death. If we had a complete set of observations this would deliver 22,491 obs, but for some states we are "missing" some methods of death.

Let us check this to make sure

```{r}
table1 <- merge_data %>% group_by(State, Year) %>%
                          summarise(n = n()) %>%  # counts number of rows in each group
                          spread(Year,n) %>% print()
```

As you can see from this table, the number of observations by State/Year is often less than 21.

Unfortunately the data series, when accessed through this database, only start in 2001. In the Siegel et al. (2019) paper data from 1991 are used (up to and including 2016). This means that here we can use data from 2001 to 2020 (last policy observation).

Before we continue there is one extra item to take care off. We already merged two datafiles and we will merge data from more datafiles a little later. When one merges on state names there is always the risk that different datasets use slightly different spelling and therefore the data may not match perfectly. This is one reason why there are codes for states and countries. In the US there are two letter codes representing states. They are contained in the `USstates` dataframe we previously loaded, but they are not yet contained in the `merge_data` dataframe. Let us add an additional column `State_code` with that two letter code.

```{r}
merge_data <- merge(merge_data, USstates)
names(merge_data)[names(merge_data)=="Code"] <- "State_code"
```

We will use this to facilitate the merging of datafiles.


## Summary Stats and Data Exploration

Let's calculate some firearm death statistics. The data from the CDC deliver an age adjusted rate which indicates the number of deaths for every 100,000 of population in state $s$ in a particular year $t$. Let's average that rate in each sate across all years.

```{r}
table2 <- merge_data %>% filter(Mechanism == "Firearm") %>% 
                          group_by(State) %>% 
                          summarise(avg_aarate = mean(Age.Adjusted.Rate,na.rm = TRUE), n=n()) %>% 
                          arrange(-avg_aarate) %>%  # sort in decreasing order of avg_aarate
                          print()
```

If you are working on replicating or extending some published work you should try and see whether your data are the same as the ones used in the publication you are working off. This is best dome by matching your own summary statistics to those in the paper. The Siegel et al (2019) paper does not really have any descriptive statistic, but report some data for 2016 (their Table 2). They report, for that year an age adjusted homicide rate of 14.2 and an age adjusted suicide rate of 14.1 in Louisiana. Let's look at our data for state state-year:

```{r}
merge_data %>% filter(Year == 2016, State == "Louisiana") %>% 
  select(State, Year, Mechanism, Age.Adjusted.Rate)
```

You will realise that the data we obtained does not actually record whether a death was a suicide or homicide, but rather we have information on the mechanism of death. Of course a death by firearm could be either a homicide or a suicide. Therefore, we really do not have the data to replicate what they have actually done. Here we will proceed by working with the firearm data.

In the table above you can see that we have a value of 21.20 (per 100,000 population) firearm deaths in Louisiana for 2016. That is not the same as the 14.2 in the paper but we wouldn't expect this as this doesn't measure homicides. But most likely it is quite highly correlated and it is certainly, size wise in the same ballpark.


Let's look at a few time-series plots of the data. Whenever you are working with data that have a time-series dimension you should do this as this will give the reader a better understanding of the data.

```{r}
plot1data <- merge_data %>% filter(Mechanism == "Firearm") %>% 
                            filter(State %in% c("Alabama","Tennessee","Maine", "New York"))
p1 <- ggplot(plot1data, aes( x = Year, y = Age.Adjusted.Rate, color = State)) +
            geom_line() +
            labs(title="Age Adjusted Death by Firearm Rates")
p1
```

There clearly seems to be an upward trend in the number of firearm deaths after 2015,but for New York in this selection of States.

## More Data Setup

In this section we will prepare the information on gun control laws in a way that makes it useable in further regression analysis. We will also load more covariates into the dataset. 

### Policy set-up

Late we wish to evaluate whether a particular gun control policy has a causal effect. For instance consider universal background checks. We can refer back to the `law_data_codebook` to see which policy codes are available. In fact there are two policy columns that relate to universal background checks.

```{r}
# the toString function makes the output easier to read. Try what happens if you don't use it
print(toString(law_data_codebook[law_data_codebook$Variable.Name == "universal","Detailed.Description.of.Provision"]))
print(toString(law_data_codebook[law_data_codebook$Variable.Name == "universalh","Detailed.Description.of.Provision"]))
```

So the difference is that the `universalh` policy only covers handguns whereas `universal` covers all firearms. We shall create a new variable that combines these two. We will create a new variable $pol_{st}$ that takes the value 1 if either of these two policies is in place in state $s$ at time $t$.

To make this happen with the tidyverse approach we have to indicate to R, before calling the definition of the new variable `mutate(pol_ubc = max(universal,universalh))` that the max here is an operation that should happen row-by-row (`rowwise()`).

```{r}
merge_data <- merge_data %>% rowwise() %>%  mutate(pol_ubc = max(universal,universalh)) # universal background checks
```

Open the merge_data object to check that your operation actually did what it was meant to do.

A different policy to consider, as it was found to have significant effects in the Siegel et al. (2019) paper, is that of a ban for weapons for people who had violent misdemeanors on their records.

```{r}
print(toString(law_data_codebook[law_data_codebook$Variable.Name == "violent","Detailed.Description.of.Provision"]))
print(toString(law_data_codebook[law_data_codebook$Variable.Name == "violenth","Detailed.Description.of.Provision"]))
print(toString(law_data_codebook[law_data_codebook$Variable.Name == "violentpartial","Detailed.Description.of.Provision"]))
```

As for universal background checks we set up a policy variable `pol_vmisd`.

```{r}
merge_data <- merge_data %>% rowwise() %>%  mutate(pol_vmisd = max(violent,violenth,violentpartial)) # universal background checks
```


In the Siegel et al. (2019) paper there was one policy which was estimated to have a positive effect on firearm deaths. This policy was labelled "Shall issue laws" and described as the "absence of any discretion of law enforcement authorities in deciding whether to grant a concealed carry permit". When scanning the `law_data_codebook` it is not obvious which entry represents this. 

You search the dataframe with the following function which finds the entries in `law_data_codebook$Detailed.Description.of.Provision` which contains the word "shall"

```{r}
grepl("shall",law_data_codebook$Detailed.Description.of.Provision)
```

If R finds a cell which contains that word it will represent this with a "TRUE" in the results. There is none, so the word "shall" is not contained in any of the descriptions.

Re-read the above description and you will realise that it actually describes the **absence** of a policy to control who should be allowed to carry a concealed weapon. With that in mind scan the policies categorised as "Concealed carry permitting" policies. You will find a policy labelled "mayissue" (in line 77). The describtion is:

```{r}
print(toString(law_data_codebook[law_data_codebook$Variable.Name == "mayissue","Detailed.Description.of.Provision"]))
```

Therefore it may well be that the policy in the Siegel et al. (2019) paper is coded as the absence of that policy (we cannot really be certain about this as the authors do not exactly describe this, although we could investigate further as in their Table 1 they also provide listings of states and number of policy changes). We could therefore code it as `1-mayissue`. However, for now we just use "mayissue" as a policy which has a potential firearm death reducing effect (rather than an increasing effect of the absence of the policy).

```{r}
merge_data <- merge_data %>% rowwise() %>%  mutate(pol_mayissue = mayissue) # universal background checks
```

There is a range of other policies that were all estimated to not have a policy impact. Of course the conclusion may change as we are looking at a different sample period. 

Before we go to estimating models it is useful to figure out how much variation we can observe in the policy in the states. What we require for a difference-in-difference setup is variation in policy in at least one state. A good way to do that is to visualise the series of 0s and 1s for a particular policy across the states. This is not a standard visualisation task, so it is likely that you may wish to search the internet for advise on "R ggplot visualise elements of a matrix". This admittedly already uses the a pre-conception that representing the 0s and 1s in a matrix is useful.

```{r}
ggplot(merge_data, aes(x = Year, y = State)) + 
  geom_raster(aes(fill=pol_ubc)) + 
  scale_fill_gradient(low="grey90", high="red") +
  labs(x="Year", y="State", title="Policy - Universal Background Check") 
```
There are two important things you can see from here.

1. There are no data for 2021 (as discussed above) and no data for the District of Columbia, reducing the number of effetive states to 50.
1. Most states do not have any universal background checks policy. Some states have this policy in place for the entire sample period (e.g. Indiana).
1. Some states have a universal background check policy in place for the entire sample period, e.g. Rhode Island.
1. Some states introduce the policy during the sample period, e.g. Virginia. This is important as a DiD estimation relies on these states for identifying the policy effect.

Let's create the similar plot for the other two policies.

```{r}
ggplot(merge_data, aes(x = Year, y = State)) + 
  geom_raster(aes(fill=pol_vmisd)) + 
  scale_fill_gradient(low="grey90", high="red") +
  labs(x="Year", y="State", title="Policy - Violent Misdemeanors") 
```

Here you can see that there is variation across states, but no variation in a state. This means that, with the given sample (only starting in 2000 due to the lack of earlier data for mechanisms of death), we cannot investigate the effectiveness of the violent misdemeanor policy. In the Siegel et al. (2019) paper they can investigate the policy as there is some variation between 1991 and 2000. Running the following code would demonstrate this (Result not shown here). 

```{r, eval = FALSE}
temp <- law_data
temp <- temp %>% rowwise() %>%  mutate(pol_vmisd = max(violent,violenth,violentpartial)) # universal background checks

ggplot(temp, aes(x = Year, y = State)) + 
  geom_raster(aes(fill=pol_vmisd)) + 
  scale_fill_gradient(low="grey90", high="red") +
  labs(x="Year", y="State", title="Policy -Violent Misdeameanor - 1991-2020") 
```

Lastly for the mayissue policy.

```{r}
ggplot(merge_data, aes(x = Year, y = State)) + 
  geom_raster(aes(fill=pol_mayissue)) + 
  scale_fill_gradient(low="grey90", high="red") +
  labs(x="Year", y="State", title="Policy - May Issue") 
```
We can see that there is variation in the policy inside states and therefore we can investigate the policy. The variation is of the type that this policy existed at the beginning of the sample, but then was withdrawn, e.g. Wisconsin or Alabama. In fact this may be the reason why Siegel et al. (2019) used the reverse of this as a policy.


## Data on covariates

The Siegel et al. (2019) paper uses a number of extra covariates that can be used in the DiD regressions.

* State unemployment data
* Number of law enforcement officers by state
* Violent crime rate (excl homicides)
* per capita alcohol consumption
* age structure of the population

We will obtain the data where we can and then create datafiles with State-Year observations such that we can merge them into the `merge_data` dataframe. For this to work smoothly, each of these datafiles should have a column called `State` with the name of the state and a column called `Year` with the year. The Spelling has to match this spelling.

### Unemployment data

Unemployment data are standard economic data and we can obtain them from [FRED](https://fred.stlouisfed.org/graph/?m=QzBC#).

```{r}
# note the skip = 1 to skip the first row, column headings are in row 2
# recall, always have a look at the original datafile
ur_data <- read.csv(paste0(datadir,"Unemployment Rate by State (Percent).csv"),skip = 1)
head(ur_data[,1:7])
```

The data are actually monthly data. And they are not in the long format (i.e. one row for a state-month observation). So there are two things we need to do with this file. 1) we need to turn it into a long format and then we need to average the unemployment rate over years. Let's start with turning this into a long format.

We start by removing the `Series.ID` variable and the `Region.Code` and renaming the `Region.Name` variable to `State`.

```{r}
ur_data <- ur_data %>% select(-Series.ID) %>% select(-Region.Code)
names(ur_data)[names(ur_data)=="Region.Name"] <- "State"
```

Now we use the `pivot_longer` function to turn the wide into a long dataset. The current column names (e.g. "X01.01.2001") is turned into values for the new variable `Date` and the values in the table are turned into a new variable `ur`. The `State` variable is left as it is (`!State` basically tells the function "Not State", meaning do not do anything with the sate variable).

It is unlikely that you will apply this function often enough to remember how this function works. You just need to know this function exists and then use the help function or examples on the internet to figure out what to do. And remember you cannot break the computer. Just try and test what does the job and what doesn't.

```{r}
ur_data_long <- ur_data %>% pivot_longer(!State, names_to = "Date", values_to = "ur")
head(ur_data_long)
```

Step 1 is done. You can now see that in the `date` column we have the monthly dates. They all have an "X" at the beginning which results from the fact that variable names in R have to start with a letter and when the data were imported R added an "X" to the column headings which were the dates. All we need from the dates is the year which in the last four digits. So what we want to extract from the `date` column is the last four digits. This is not an operation you come across very often and hence a web-search sounds like the way to go here. Go to your favourite search engine and search for something like "R extracting last characters".

You may find different pieces of advice as there are many different ways to achieve this. Here I adopt a method which uses the `str_sub` function (Check out the help function by typing `?str_sub` into the console.)

```{r}
ur_data_long$Year <- as.numeric(str_sub(ur_data_long$Date,-4))
head(ur_data_long)
```

As you can see, this has now created a `Year` column. All we got to do now is to calculate an average unemployment rate for each state in each year.

```{r}
ur_data_long <- ur_data_long %>% group_by(State, Year) %>% 
                                    summarise(ur = mean(ur))
```

Let's plot some data to confirm that we have obtained sensible data.

```{r}
plot1ur <- ur_data_long %>% 
                            filter(State %in% c("Alabama","Tennessee","Maine", "New York"))
p2 <- ggplot(plot1ur, aes( x = Year, y = ur, color = State)) +
            geom_line() +
            labs(title="Unemployment rate")
p2

```

This plot looks reasonable showing unemployment rate peaks in the aftermath of the financial crisis and Covid. `ur_data_long` is now ready to be merged with `merge_data`.

```{r}
merge_data <- merge(merge_data,ur_data_long, all.x = TRUE)
```

### Law enforcement officer numbers

The F.B.I. has a Crime Data Explorer and in there you can find a number of crime related data, amongst others the [Law Enforcement Employees Data](https://cde.ucr.cjis.gov/LATEST/webapp/#) which provides a detailed breakdown of law enforcement employees for very small geographies like cities. As the datafile contains state information we can aggregate these to state level data.

The data are saved in "law enforcement employees FBIdata_1960_2022.csv".

```{r}
officers_data <- read.csv(paste0(datadir,"law enforcement employees FBIdata_1960_2022.csv"))
```

Have a look at the spreadsheet to get an idea of the data. There are more than 700,000 rows of data. One row represents the information for one agency in a particular year. But when inspecting the datafile you would have seen that there are loots of agencies.

Let's have a look at one particular row of data to get a feel for the data.
```{r}
officers_data[99,]
```

Let's start by changing the names of the State and Year variables.

```{r}
names(officers_data)[names(officers_data)=="data_year"] <- "Year"
names(officers_data)[names(officers_data)=="state_abbr"] <- "State_code"
```

As we need the number of officers in a state in one year we need to aggregate over all agencies in a particular year in a state. We also restrict our attention to years 2001 onwards.

```{r}
officers_data_agg <- officers_data %>% filter(Year >= 2001) %>% 
                                      group_by(State_code, Year) %>% 
                                      summarise(law.officers = sum(total_pe_ct))
```

Let's look at the data to get a feel for the information in these data.

```{r}
plot1off <- officers_data_agg %>% 
                            filter(State_code %in% c("AL","TN","ME", "WV"))
p2 <- ggplot(plot1off, aes( x = Year, y = law.officers, color = State_code)) +
            geom_line() +
            labs(title="Number of Law Enforcement Officers")
p2

```

You can see that the data in `officer_data_agg` are absolute numbers and we will have to scale them to number of officers per 100,000 population. We will do this after merging the data into `merge_data`.

```{r}
merge_data <- merge(merge_data,officers_data_agg, all.x = TRUE)
```

As it turns out that there are no officers data for West Virginia for the years 2008 and 2014, it was important to add the option ` all.x = TRUE` to ensure that these two years remain preserved for 2008 and 2014 in the merged dataset. Without this option, these two observations would be removed from the `merge_data` dataframe.

Before continuing we shall scale this variable by the population to calculate how many law enforcement officers a state has per 100,000 population.

```{r}
merge_data$law.officers.pc <- 100000*merge_data$law.officers/merge_data$Population
```

### Violent crime rate

From the F.B.I. website you can access the [Data Discovery Tool](https://cde.ucr.cjis.gov/LATEST/webapp/#/pages/home) which is an interface which gives you access for a range of data. There are four categories of violent crime, homicide, rape, robbery and aggravated assault. Tick the last three and then download the data state by state for the years 2001 to 2021. ("add query" to add up to 5 states at a time, then submit, show the table and download; then reset query and chose the next 5 states).

image: ![](FBI_CDT.jpg)

While doing this for every state is a chore (although you can do it for 5 states at a time) it is ok as the resulting tables are all formatted identically and therefore it is fairly straightforward to get all data into one table.

In fact, when you download data like this, it may be wise to download more data than you now think you need. For instance you could download data back to 1991 (the Siegel sample) just in case you were to find the additional earlier years of firearm deaths. The data are available from "Violent crime excl homicide by State.xlsx".

```{r}
vcrime_data <- read_excel(paste0(datadir,"Violent crime excl homicide by State.xlsx"))
head(vcrime_data)
```

You can see that the data are not arranged in the format which we need, i.e. one row for each State-Year. Rather we have years in rows and states in columns. We shall again apply the `pivot_longer` function. 

```{r}
vcrime_data_long <- vcrime_data %>% pivot_longer(!Year, names_to = "State_code", values_to = "vcrime")
head(vcrime_data_long)
```

These are now in the right format for merging. Again we have number of crimes and we will want to transform it to a per 100,000 population measure once we merged this number into `merge_data`.

```{r}
merge_data <- merge(merge_data,vcrime_data_long, all.x = TRUE)
```

Before continuing we shall scale this variable by the population to calculate how many law enforcement officers a state has per 100,000 population.

```{r}
merge_data$vcrime.pc <- 100000*merge_data$vcrime/merge_data$Population
```


### Homicide data

As exploring where to get the violent crime data we eventually also discovered a source for the homicide data as that was the fourth category of violent crime. With the same process (as for other violent crime) we can download the state by state homicide data. They have been collated in the "Homicide by State.xlsx" file.

```{r}
homi_data <- read_excel(paste0(datadir,"Homicide by State.xlsx"))
head(homi_data)
```

As for the violent crime data we will go through the same process of applying `pivot_longer`, then merging with `merge_data` and then calculating the homicide rate per 100,000 population.


```{r}
homi_data_long <- homi_data %>% pivot_longer(!Year, names_to = "State_code", values_to = "homi")
merge_data <- merge(merge_data,homi_data_long, all.x = TRUE)
merge_data$homi.pc <- 100000*merge_data$homi/merge_data$Population
```

Do we now have the homicide data used in the Siegel et al (2019) paper. Let's see what the value for Louisiana is in 2016. In the paper that was 14.2.

```{r}
merge_data %>% filter(State == "Louisiana", Year == 2016) %>% select(State, Year, homi.pc) %>% unique()
```

As you can see the value is not exactly the same. The reason is that the data we got from the F.B.I. website are not age adjusted data (as are the data from the CDC). This means that in the later analysis we can either use the age adjusted forearm deaths data or the non-age adjusted homicide data. We could of course also attempt to apply the age adjustment procedure, but we will not do this here.


### Per-capita alcohol consumption

The Siegel paper indicates that two different sources were used to obtain a full set of data. One of the sources was the National Institute on Alcohol Abuse and Alcoholism. I therefore performed a web search for "National institute on alcohol abuse and alcoholism state data" and was directed to [Surveillance Report webpage](https://www.niaaa.nih.gov/publications/surveillance-reports) which seemed to contain under Surveillance Report #120 exactly what was needed. Under that item there was a link to a [datafile](https://www.niaaa.nih.gov/sites/default/files/pcyr1970-2021.txt) which seemed to contain exactly the information needed.

This is a text file (it is best opened with a basic text editor, in Windows there is one called Notepad, on a Mac there is one called TextEdit). At the beginning of the file you will see information on the variables contained in the file. Here we have converted this text file into a csv file (which you can achieve by copying and pasting the portions of the file that actually contain the data into Excel and then applying the "Text to columns" function).

```{r}
alcc_data <- read.csv(paste0(datadir,"Alcohol consumption by US state.csv"))
head(alcc_data)
```

You can see that here, the states are not indexed by their name, nor by their two letter code, but rather by a number. So to be able to merge this with the remaining data we need to introduce either the name or the two letter code. At the top of the text file linked above you can see which number relates to which state. This is a standard state numbering, E.g. Wyoming is state 56, and this numbering was actually contained in the US States dictionary we uploaded earlier under the `State_num_code` variable which was also merged into the `merge_data` dataset.


We adjust the state number variable in `alcc_data` to the name of the numeric code column in `merge_data` such that we can match on these.

```{r}
names(alcc_data)[names(alcc_data)=="State.no"] <- "State_num_code"
```

Finally we should identify the variable which we want to merge into `merge_data`. The Siegel et al. (2019) paper does not clearly state which series was to use, but for now we use the variable called `Gallons.of.ethanol.per.capita.age.14.and.older` and will then rename this variable to `alcc_pc`. Finally, upon inspecting the dataframe you will see that for each State-Year there are 4 lines with observations for beverage types 1, 2, 3 and 4. The data description in the earlier text file comes to help to identify  1:Spirits, 2: Wine, 3: Beer and 4: All beverages. We chose `Type.of.beverage == "4"`.

```{r}
names(alcc_data)[names(alcc_data)=="Gallons.of.ethanol.per.capita.age.14.and.older"] <- "alcc.pc"
alcc_data <- alcc_data %>% filter(Type.of.beverage == "4") %>% 
                  filter(Year >= 2001) %>% 
                  select(State_num_code, Year, alcc.pc) %>% 
                  mutate(alcc.pc = alcc.pc/10000) %>% 
                  arrange(State_num_code, Year)
```

Note that we divided the alcc.pc variable by 10,000. As described in the text file, this scales the variable to gallons per person. This file is ready for merging to `merge_data`.

```{r}
merge_data <- merge(merge_data,alcc_data, all.x = TRUE)
```

### Incarceration rate

The paper says that the data come from the U.S. Bureau of Justice Statistics website. If we search for that we get to [this website](https://bjs.ojp.gov/). Using the search function from that website you do fairly quickly find your way to annual reports like [this one for 2021](https://bjs.ojp.gov/library/publications/prisoners-2021-statistical-tables) from where you can download the data tables of the report, one of which is the table which lists the number of prisoners by state. However, having to find this table for all years is a rather burdensome endeavour. Furthermore, there is no guarantee that the tables, across the years, have stayed in the same format and therefore collating a complete dataset in this way is unlikely to be straightforward.

On that page is a "Related Datsets" section and there is a link labeled "National Archive of Criminal Justice Data's (NACJD) 1978-2021". If you follow that link you get to [this website](https://www.icpsr.umich.edu/web/NACJD/studies/38555) from where you can download a complete dataset. I downloaded the ASCII version and converted that to a "csv" file which we can upload here as "Incarceration data.csv". The file has data organised in State/Years by row. There are a lot (231) variables and as you download the data you should also download the codebook which explains what the variables are. From that document you can see that you can get information on incarceration numbers by gender and length of maximum sentence. There is a variable that gives the total number of person's incarcerated in a state at 31 December of any given year. This variable is labelled `CUSTTOT`. While the Siegel et al. (2019) paper is unclear on what number they use this seems the most likely and we will use it here.

```{r}
# we also specify codes for missing data
incarceration_data <- read.csv(paste0(datadir,"Incarceration data.csv"),na.strings = c("-9" , "-8" , "-2" , "-1"))
```


Let's look at a few plots to become familiar with the data

```{r}
plot1inc <- incarceration_data %>% 
                            filter(STATE %in% c("AL","TN","ME", "NY"))
p2 <- ggplot(plot1inc, aes( x = YEAR, y = CUSTOTT, color = STATE)) +
            geom_line() +
            labs(title="Incarceration Numbers")
p2
```

As you can see from the plot, there are only data up to 1982. To investigate what is going on we shall look at data for one state, say Alabama (AL).

```{r}
test <- incarceration_data %>% filter(STATE == "AL")
```

If you look at the resulting table you will realise that apparently the variables that were collected changed between 1982 and 1983. In particular, the breakdown by gender is only available from 1983 onwards. The variables that may be of interest to us (after 1983) are `CUSTOTM` and `CUSTOTF` which capture the male and female totals in custody. The sum of the two should then represent the total number of people in custody. We therefore may now want to calculate `CUSTOTT` as the sum of these two for 1983 onwards.

```{r}
incarceration_data <- incarceration_data %>% mutate(CUSTOTT = case_when(
                                                  !is.na(CUSTOTT) ~ CUSTOTT,
                                                  is.na(CUSTOTT) ~ CUSTOTF+CUSTOTM)) 
```

You should look at the dataset to confirm that we now have data in the `CUSTOTT` column for all years. We can also confirm this graphically.

```{r}
plot1inc <- incarceration_data %>% 
                            filter(STATE %in% c("AL","TN","ME", "NY"))
p2 <- ggplot(plot1inc, aes( x = YEAR, y = CUSTOTT, color = STATE)) +
            geom_line() +
            labs(title="Incarceration Numbers")
p2
```

This looks good and in particular we cannot see any obvious structural break between 1982 and 1983, giving us confidence that our calculation is sensible.

Now we extract the data we will need for our analysis and which we will merge into our `merge_data` dataframe.

```{r}
incarceration_data <- incarceration_data %>% filter(YEAR >= 2001) %>% 
                          select(YEAR, STATE, CUSTOTT)
```

Finally we will have to adjust the column names for the state code and the year to match those in `merge_data` to facilitate the merging process. We also change the name of the `CUSTOTT` variable.

```{r}
names(incarceration_data)[names(incarceration_data)=="YEAR"] <- "Year"
names(incarceration_data)[names(incarceration_data)=="STATE"] <- "State_code"
names(incarceration_data)[names(incarceration_data)=="CUSTOTT"] <- "incarc"
```

As you can see these are the total number of person incarcerated. We will want to scale this to a number like person per 100,000 population. We can do that after merging as `merge_data` contains a variable which contains the size of the population in a state in a particular year.

```{r}
merge_data <- merge(merge_data,incarceration_data, all.x = TRUE)
```

We now scale the variable to represent the number of people incarcerated for every 100,000.

```{r}
merge_data$incarc.pc <- 100000*merge_data$incarc/merge_data$Population
```



### Age proportion

A variable that is used in the paper is the age structure of a state's population. In the Siegel et al. (2019) paper you will find that they use a variable called "Percent male among population ages 15-29". We shall attempt to use a different variable "Proportion of 18-24 year olds in the population". You will see below that adding this data to our datasets require a bit of work. With enough work we could add the data used in the paper, but for today's exercise we will make our life a little easier. 

We shall import a new datafile that contains some of that information. The data are sourced from the [StatsAmerica website](https://www.statsamerica.org/downloads/default.aspx). The "US states population age and sex.csv" file should be saved into your working folder.


```{r, echo = FALSE}
data_pop <- read.csv(paste0(datadir,"US states population age and sex.csv"), stringsAsFactors = TRUE)    
str(data_pop)
```

This file has 63882 rows. How many would you have expected if there were 51 states and 21 years for each state? 1,071 rows. The spreadsheet includes data for every county and not only for the whole state of Alabama. For instance you see that some rows have in the description column only the name of a state and others have the name of a county. To illustrate this look at the following snippet from the data table:

```{r}
data_pop[1979:1982,]
```

Note that in lines 1979 and 1980 we are having data for the whole state of Arizona and in lines 1981 and 1982 you find data for Apache County (in Arizona). We only want statewide data. In the above snippet you can see that there is a variable called `Countyfips` which is a numerical code for the different counties. The statewide data have a value of 0 in the `Countyfips` variable. You should confirm (by looking at the data) that this is true for the other states as well.

One additional aspect of the data is that you will see that population data are only available from 2000 to 2019. This is not aligned with the 2001 to 2021 date range in `data`. The common years are 2001 to 2019 and therefore we should expect to get 969 (=51*19) observations which we can match.

Let us first filter out the statewide data and remove the county level data.

```{r}
data_pop <- data_pop %>% filter(Countyfips == 0)  # we only keep data with Countyfips equal to 0
```

You will notice that this dataframe now has `nrow(data_pop2)` rows of data. This is still too many rows. Let's look at the different geographies in our dataset.

```{r}
unique(data_pop$Description)
```
You will immediately see that there are also observations for the entire U.S.. So, let's extract the data that are from states and years which are also represented in `merge_data`, our original dataset. Complete the following code for this task.

```{r, echo = FALSE}
state_list <- unique(merge_data$State)  # creates a list with state names in data
year_list <- unique(merge_data$Year)    # creates a list of years in data
data_pop <- data_pop %>% filter(Description %in% state_list) %>% 
                            filter(Year %in% year_list)
```

You got it right if `data_pop` has 969 observations and you can replicate the following table:

```{r}
summary(data_pop$Total.Population)
```
Let's look at the variables that are contained in this datafile.

```{r}
names(data_pop)
```

We shall not merge all of these variables into `data` but only what we want, namely the "Proportion of 18-24 year olds in the population". That is actually not one of the variables in the list. There is the population between 18 and 24 (`Population.18.24`) and the overall population (`Total.Population`) and we can calculate the proportion we need as a new variable, `prop18.24`. Complete the following code:

```{r, echo = FALSE}
data_pop$prop18.24 <- 100*data_pop$Population.18.24/data_pop$Total.Population 
```

You get it right if you can replicate these summary statistics for the new variable.

```{r}
summary(data_pop$prop18.24)
```
Now we select only the variables we wish to merge into `data`, namely only `prop18.24`. However, in order to merge the data into `data` we also need the year (`Year`) and state name (`Description`).

```{r}
data_pop <- data_pop %>%  select(Year, Description, prop18.24)
```

It is easiest to merge datafiles if the variables on which we want to match (state name and Year) are called the same in both datasets (`merge_data` and `data_pop`). This is true for the `Year` variable, but not for the state name (`State` in `merge_data` and `Description` in `data_pop`). Let's fix that and change the state variable name in `data_pop` to `State`.

```{r}
names(data_pop)[names(data_pop)=="Description"] <- "State"
```

Now we are in a position to merge the two datafiles.

```{r}
merge_data <- merge(merge_data,data_pop, all.x = TRUE)
```

As result your datafile has gained one variable, `prop18.24`.

## Final data prep

We have now assembled all the data. Recall that at this stage we still have all the mechanisms of death in the dataset. That means that for every year state we still have up to 21 observations. Let's racall the different mechanisms of death.

```{r}
unique(merge_data$Mechanism)
```

At this stage we will extract only the `firearm` data and continue working with these.

```{r}
data3 <- merge_data %>% filter(Mechanism == "Firearm")
```

As expected we get $51 8 21 = 1071$ observations. Let us also rename the variable `Age.Adjusted.Rate` which contains the age adjusted rate of firearm deaths (per 100,000). The new name will be `firearm.deaths.pc`.

```{r}
names(data3)[names(data3)=="Age.Adjusted.Rate"] <- "firearm.deaths.pc"
```

The datafile also has loads of variables. You can check the whole list with `names(data3)`. In principle there is no problem with keeping them all in the datafile as there are not that many observations that the memory in your computer would be challenged. But sometimes it is nicer to work with a compact dataset. For this reason we will select the variables we are likely to use in further analysis.

```{r}
data3 <- data3 %>% select(Year, State, State_code, State_num_code, Population, firearm.deaths.pc, Region, pol_ubc, pol_vmisd, pol_mayissue, ur, law.officers.pc,vcrime.pc, homi.pc, alcc.pc, incarc.pc,prop18.24)
```

Now we have a dataset with 1,071 rows and 17 variables. Let us save this into a csv file such that for further work we can just start with that new csv file.

```{r}
write.csv(data3,paste0(datadir,"US Gun_example_v3.csv"))
```
