---
title: "Introduction to Handling Data"
subtitle: "ECON20222 - Lecture 2"
author: "Ralf Becker and Martyn Andrews"
date: "January 2019"
output: 
  beamer_presentation:
    includes:
      in_header: ../latex_template.tex
#     in_header: ../latex_student.tex  # use for student version
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## Aim for today

\begin{itemize}
  \item Review hypothesis testing
  \item Review simple regresison anlysis
  \item Become more familiar with R
\end{itemize}



## Preparing your workfile

We add the basic libraries needed for this week's work: 

```{r, echo = TRUE, error = FALSE, message = FALSE, results = FALSE, warning = FALSE, eval=TRUE}
library(tidyverse)    # for almost all data handling tasks
library(readxl)       # to import Excel data
library(ggplot2)      # to produce nice graphiscs
library(stargazer)    # to produce nice results tables
```


## New Dataset - Wellbeing

[Doing Economics Project 8](https://www.core-econ.org/doing-economics/book/text/08-03.html) deals with international wellbeing data.

Data are from the [European Value Survey](https://europeanvaluesstudy.eu/). 

* A large catalogue of questions on Perceptions of Life, Politics and Society, Work, Religion etc
* 48 mainly European countries
* Four waves/years of data (1981, 1990, 1999 and 2008)
* 129,515 observations (people/respondents) 

```{r, echo = TRUE, error = FALSE, message = FALSE, results = FALSE, warning = FALSE, eval=TRUE}
load("WBdata.Rdata")
```

This will load two objects into your environment

* `wb_data` - the actual data file
* `wb_data_Des` - a table which contains some description to each variable

To get to this dataset a significant amount of data handling and cleaning had to happen (see Project 8 in Doing Economics.)

## Wellbeing Data
\scriptsize
```{r, echo = TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE}
str(wb_data)  # prints some basic info on variables
```
\normalsize


## Data Description

\tiny
```{r, echo = TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE}
wb_data_Des  # prints some basic info on variables
```
\normalsize

## Data - Countries

Let's find out which countries are in the sample:

\scriptsize
```{r, echo = TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE}
unique(wb_data$S003)   # unque finds all the different values in a variable
```
\normalsize
\textcolor{student}{Point out what unique does.}

## Data - Waves

Let's find out how many observations/respondents we have for each country (`S003`) in each wave (`S002EVS`). 

Use piping technique of the `tidyverse`
\tiny
```{r, echo = TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE}
table1 <- wb_data %>% group_by(S002EVS,S003) %>% # groups by Wave and Country
      summarise(n = n()) %>%               # calculating no of obs
      spread(S002EVS,n) %>%                # put Waves across columns
      print(n=4)                         
```
\normalsize

For each country ($j=1,...,48$) we have observations from potentially four years ($t=1,...,4$). For each country-year ($jt$) we have ($i=1,...,n_{jt}$) observations, e.g. $n_{Austria, 1990}=1432$. Each observation can be identified/indexed by $ijt$. \textcolor{student}{Repeated Cross-Section}

## Data - Some graphical representation

Summarise data by country and wave. 

* A170: All things considered, how satisfied are you with your life as a whole these days? (1 Dissatisfied to 10 Satisfied)
* A009: All in all, how would you describe your state of health these days? Would you say it is ... 1 Very good to 5 Very poor

\scriptsize
```{r, echo = TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE}
table2 <- wb_data %>% group_by(S002EVS,S003) %>% # groups by Wave and Country
            summarise(Avg_LifeSatis = mean(A170),Avg_Health = mean(A009))     
head(table2,4)
```
\normalsize

\textcolor{student}{table2 now contains an observation for each country-year with Avg\_LifeSatis and Avg\_Health}


## Data - Some graphical representation

\tiny
```{r, echo = TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE}
ggplot(table2,aes(Avg_Health,Avg_LifeSatis, colour=S002EVS)) +
  geom_point() +
  ggtitle("Health v Life Satisfaction")
```
\normalsize

## Data - Some graphical representation

Summarise data by country and wave. 

* A170: All things considered, how satisfied are you with your life as a whole these days? (1 Dissatisfied to 10 Satisfied)
* C041: Work should come first even if it means less spare time, 1 = Strongly Agree, 5 = Strongly Disagree

\scriptsize
```{r, echo = TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE}
table2 <- wb_data %>% group_by(S002EVS,S003) %>% 
            summarise(Avg_LifeSatis = mean(A170),Avg_WorkFirst = mean(C041))   
```
\normalsize

## Data - Some graphical representation

\tiny
```{r, echo = TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE}
ggplot(table2,aes( x=Avg_WorkFirst, y=Avg_LifeSatis,colour=S002EVS)) +
  geom_point() +
  ggtitle("Work First v Life Satisfaction")
```
\normalsize

## Data - Some graphical representation

There seems to be a negative relationship between Attitude to Work ($WC$) and Life Satisfaction ($LS$) (in countries with a "work-centric" ethic people were on average happier.)

Is there such a relationship inside countries as well?

We will calculate correlations for each country-wave, e.g. Austria in 2008:

\scriptsize
$Corr_{Aut,2008}(LS_{i,Aut,2008},WC_{i,Aut,2008})$ \textcolor{student}{ $=\dfrac{Cov_{Aut,2008}(LS_{i,Aut,2008},WC_{i,Aut,2008})}{s_{LS,Aut,2008}~s_{WC,Aut,2008}}$}

```{r, echo = TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE}
table3 <- wb_data %>% filter(S002EVS == "2008-2010") %>% 
            group_by(S003) %>% # groups by Country
            summarise(cor_LS_WF = cor(A170,C041,use = "pairwise.complete.obs"),
                      med_income = median(X047D)) %>%    
            arrange(cor_LS_WF) 
```
\normalsize

\textcolor{student}{Point out that correlations are in $[-1,1]$. They are standardised covariances. Ensure you revise how to calculate sample s.d. and covariances!}

## Data - Some graphical representation

\tiny
```{r, echo = TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE,fig.height = 2, fig.width = 3}
ggplot(table3,aes( cor_LS_WF, med_income)) +
  geom_point() +
  ggtitle("Corr(Life Satisfaction, Work First) v Median Income")
```
\normalsize

\textcolor{student}{Note that the correlation between $LS$ and $WC$ is close to 0 in most countries}


## Data on Maps

Geographical relationships are sometimes best illustrtaed with maps.

Sometimes these will reveal a pattern.

R can create great maps (but it requires a bit of setup - see the additional file on BB). You need the following

* A shape file for each country
* The statistics for each country
* a procedure to merge these bits of information in one data-frame (`merge`)

Let's look at average life satisfaction average attitude to the "work first" statement as these statistics vary by country.

## Data on Maps

```{r, echo = FALSE, eval=TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE,fig.height = 3, fig.width = 4.5}
library(tmap)   # mapping package
library(sf)     # required to deal with shape files
library(spData) # delivers shape files
wb_data_map <- wb_data   # duplicate the dataset so we keep the original unchanged
wb_data_map$S003[wb_data_map$S003 == "Bosnia Herzegovina"] <- "Bosnia and Herzegovina"
wb_data_map$S003[wb_data_map$S003 == "Great Britain"] <- "United Kingdom"
count_list <- unique(wb_data_map$S003)
count_list <- count_list[!(count_list %in% c("United States","Canada","Russian Federation"))]
d_world <- world
d_sel <- d_world %>% 
            filter(name_long %in% count_list)
table2_map <- wb_data_map %>% group_by(S002EVS,S003) %>% # groups by Wave and Country
            summarise(Avg_LifeSatis = mean(A170),Avg_WorkFirst = mean(C041))    # summarises each group by calculating obs
d_sel_merged <- merge(x = d_sel,y = table2_map,by.x = "name_long",by.y="S003") # merge the data in x and y
d_sel_2018 <- d_sel_merged %>% filter(S002EVS == "2008-2010")

map1 <- tm_shape(d_sel_2018) +
  tm_borders() +
  tm_fill(col = "Avg_LifeSatis", title = "Satisfaction with your life") +
  tm_style("cobalt") +
  tm_layout(title = "Average Life Satisfaction") 

map2 <- tm_shape(d_sel_2018) +
  tm_borders() +
  tm_fill(col = "Avg_WorkFirst", title = "1 = Strongly Agree, 5 = Strongly Disagree") +
  tm_style("cobalt") +
  tm_layout(title = "Average Attitude to Work First") 

tmap_arrange(map1, map2)   # this arranges the maps next to each other
```



## Hypothesis Testing - Introduction

Hypothesis testing is a core technique used in empirical analysis. Use sample data to infer something about the population mean (or correlation, or variance, etc). Hence \emph{inference}.

It is crucial to understand that the particular sample we have is one of many different possible samples. Whatever conclusion we arrive at is not characterised by certainty.

**Example**

Are average life satisfaction in Germany and Britain the same? (for 2008-2010 wave)
\vskip -1cm
\begin{eqnarray*}
  H_0&:& \mu_{LS,Ger,2008} = \mu_{LS,GB,2008}\\
  H_A&:& \mu_{LS,Ger,2008} \neq \mu_{LS,GB,2008}
\end{eqnarray*}

\textcolor{student}{The truth is either represented by $H_0$ or $H_A$}

When performing a test we need to calibrate some level of uncertainty. We typically fix the Probability with which we reject a correct null hypothesis (Type I error). This is also called the significance level.


## Hypothesis Testing - Introduction

Depending on the type of hypothesis there will be a \textcolor{blue}{test statistic} which will be used to come to a decision.

**Assuming that $H_0$ is true** this test statistic has a random distribution (frequently t, N, $\chi^2$ or F). We can then use this distribution to evaluate how likely it would have been to get the type of sample we have if the null hypothesis was true (\textcolor{student}{~~p-value~~}) or obtain \textcolor{student}{critical values}.

**Decision Rule 1**: If that probability is smaller than our pre-specified significance level, then we \textcolor{student}{~~reject~~} \textcolor{red}{$H_0$}. If, however, that p-value is larger than our pre-specified significance level then we will \textcolor{student}{~~not reject~~} \textcolor{red}{$H_0$}.

**Decision Rule 2**: If the absolute value of the test statistic is larger than the critical value (obtain from the Null distribution - see next slide), then we \textcolor{student}{~~reject~~} \textcolor{red}{$H_0$}. If, however, the absolute value of the test statistic is smaller than the critical value, then we will \textcolor{student}{~~not reject~~} \textcolor{red}{$H_0$}.


## Hypothesis Testing - Introduction

**Example**
The test statistic

\[t = \dfrac{\bar{LS}_{Ger,2008} - \bar{LS}_{GB,2008}}{\sqrt{\frac{s_{LS,Ger,2008}}{n_{Ger,2008}}+\frac{s_{LS,GB,2008}}{n_{GB,2008}}}}\]

How is this test statistic, \textcolor{blue}{t}, distributed (assuming $H_0$ is true)? \textcolor{red}{**If**}

1. The two samples are independent
2. The random variables $LS_{i,Ger,2008}$ and $LS_{i,GB,2008}$ are either normally distributed or we have sufficiently large samples
3. The variances in the two samples are identical

then $t \sim$ \textcolor{student}{$t$ distributed 
with $(n_{Ger,2008}+n_{GB,2008}-2)$ degrees of freedom.}

The above assumptions are crucial (and they differ from test to test). If they are not met then the resulting p-value (or critical values) are not correct. \textcolor{student}{Other tests will have different distributions and require different assumptions!}

## Hypothesis Testing - Example 1

Let's create a sample statistic:

\scriptsize

```{r, echo = TRUE, eval=TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE}
test_data_G <- wb_data %>% 
  filter(S003 == "Germany") %>%     # pick German data
  filter(S002EVS == "2008-2010")    # pick latest wave
mean_G <- mean(test_data_G$A170)

test_data_GB <- wb_data %>% 
  filter(S003 == "Great Britain") %>%  # pick British data
  filter(S002EVS == "2008-2010")       # pick latest wave
mean_GB <- mean(test_data_GB$A170)

sample_diff <- mean_G - mean_GB
sample_diff
```
\normalsize

Is this different \textcolor{student}{~~~statistically and/or economically~~~} significant?

## Hypothesis Testing - Example 1

Formulate a null hypothesis. Here that the difference in population means (`mu`) is equal to 0 using the `t.test` function. We deliver the `A170` series for oth countries to `t.test`.

\scriptsize

```{r, echo = TRUE, eval=TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE}
t.test(test_data_G$A170,test_data_GB$A170, mu=0)  # testing that mu = 0
```

\normalsize

The p-value is very small and hence \textcolor{student}{it is very unlikely that this difference would have arisen by chance if the null hypothesis WAS correct.}

## Hypothesis Testing - Example 2

What about the difference between Great Britain and Sweden though?
\scriptsize
```{r, echo = TRUE, eval=TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE}
test_data_SW <- wb_data %>% 
  filter(S003 == "Sweden") %>%  # pick British data
  filter(S002EVS == "2008-2010")       # pick latest wave

t.test(test_data_SW$A170,test_data_GB$A170, mu=0)  # testing that mu = 0
```
\normalsize
The p-value is 0.1251 and hence \textcolor{student}{there is a 12.5\% probability of this or a more extreme difference arising if the null hypothesis was true.}


## Hypothesis Testing - To reject or to not reject

\scriptsize

When comparing between Germany and Britain the p-value was app. 0: \textcolor{student}{Reject $H_0$}

When comparing between Sweden and Britain the p-value was 0.1251: \textcolor{student}{hmmmm....}

* Conventional significance levels are 10\%, 5\%, 1\% or 0.1\%
* But what do they mean?

To illustrtate we add a random variable (`rvar`) to all observations. The value comes from the identical distribution for all observations, the standard normal ($N(0,1)$ or `rnorm` in R):

```{r, echo = TRUE, eval=TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE}
wb_data$rvar <- rnorm(nrow(wb_data))   # add random variable

test_data <- wb_data %>% 
  filter(S002EVS == "2008-2010")       # pick latest wave

countries <- unique(test_data$S003)  # List of all countries
n_countries <- length(countries)   # Number of countries, 46
```

By construction we know that the true underlying mean is identical in all countries. 

But what happens if we calculate sample means of `rvar` in all countries and then compare between countries? \textcolor{student}{$(46*46-46)/2=1035$ comparisons}

\normalsize

## Hypothesis Testing - To reject or to not reject

\scriptsize

```{r, echo = FALSE, eval=TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE}
save_pvalue <- matrix(NA,n_countries,n_countries)

for (i in seq(2,n_countries)){
  for (j in seq(1,(i-1))){
    test_data_1 <- test_data %>% 
    filter(S003 == countries[i]) 
    mean_1 <- mean(test_data_1$A170)

    test_data_2 <- test_data %>% 
    filter(S003 == countries[j]) 
    mean_2 <- mean(test_data_2$A170)
    
    tt <- t.test(test_data_1$rvar,test_data_2$rvar, mu=0)  # testing that mu = 0
    save_pvalue[i,j] <- unlist(tt["p.value"])    # this will just pick the p-value
  }
}

tre <- (save_pvalue<0.1)   # value of TRUE if pvalue < 0.1
cols <- c("TRUE" = "#FFFFFF","FALSE" = "#66FF33")
```

We have 1035 hypothesis tests, all testing a \textcolor{red}{correct}. If a p-value is smaller than 10\% we \textcolor{red}{decide} to reject $H_0$.

```{r, echo = FALSE, eval=TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE}
table(tre)
```

\vskip -2cm

\begincols
  \begincol{0.3\textwidth}
    Let's present a graphical representation of these results. Every green square representing a rejection of the null hypothesis.
  \endcol
  \begincol{0.7\textwidth}
    ```{r, echo = FALSE, eval=TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE,fig.height = 4, fig.width = 4.5}
    image(1:nrow(tre), 1:ncol(tre), as.matrix(tre), col=cols)
    ```
  \endcol
    
\endcols

Is this what we would expect?
\normalsize

## Hypothesis Testing - To reject or to not reject

\scriptsize

Let's return to the Life Satisfaction data and repeat the above comparison between the average Life Satisfaction.

```{r, echo = FALSE, eval=TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE}
save_pvalue <- matrix(NA,n_countries,n_countries)

for (i in seq(2,n_countries)){
  for (j in seq(1,(i-1))){
    test_data_1 <- test_data %>% 
    filter(S003 == countries[i]) 
    mean_1 <- mean(test_data_1$A170)

    test_data_2 <- test_data %>% 
    filter(S003 == countries[j]) 
    mean_2 <- mean(test_data_2$A170)
    
    tt <- t.test(test_data_1$A170,test_data_2$A170, mu=0)  # testing that mu = 0
    save_pvalue[i,j] <- unlist(tt["p.value"])    # this will just pick the p-value
  }
}

tre <- (save_pvalue<0.1)   # value of TRUE if pvalue < 0.1
cols <- c("TRUE" = "#FFFFFF","FALSE" = "#66FF33")
```


```{r, echo = FALSE, eval=TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE}
table(tre)
```

\vskip -2cm

\begincols
  \begincol{0.3\textwidth}
    Every green square representing a rejection of the null hypothesis. \textcolor{student}{But are all these differences substantially/economically significant? Most likely not.}
  \endcol
  \begincol{0.7\textwidth}
    ```{r, echo = FALSE, eval=TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE,fig.height = 4, fig.width = 4.5}
    image(1:nrow(tre), 1:ncol(tre), as.matrix(tre), col=cols)
    ```
  \endcol
    
\endcols

\normalsize


## Regression Analysis - Introduction

Tool on which most of the work in this unit is based

* Allows to quantify relationshis between \textcolor{blue}{2 or more} variables
* It can be used to implement hypothesis tests
* However it does \textcolor{red}{not necessarily deliver causal relationships}!

It is very easy to compute for everyone! Results will often have to be interpretated very carefully.

Your skill will be to interpret correctly!!!!

## Regression Analysis - Example 1

Let's start by creating a new dataset which only contains the British data. 

```{r, echo = TRUE, eval=TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE}
test_data <- wb_data %>% 
  filter(S003 =="Great Britain") %>%  # pick British data
  filter(S002EVS == "2008-2010")         # pick latest wave
```

Now we run a regresison of the Life Satisfaction variable (`A170`) against a constant only.

$LifeSatis_{i} = \alpha + u_{i}$

```{r, echo = TRUE, eval=TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE}
mod1 <- lm(A170~1,data=test_data)
```

## Regression Analysis - Example 1

We use the `stargazer` function to display regression results

\scriptsize

```{r, echo = FALSE, eval=TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE}
stargazer(mod1, type="text")
```
\normalsize

The estimate for the constant, $\widehat{\alpha}$, is the sample mean.

## Regression Analysis - Example 1

Testing $H_0: \mu_{A170}=0$ can be achieved by
\scriptsize
```{r, echo = FALSE, eval=TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE}
t.test(test_data$A170, mu=0)  # testing that mu = 0
```
\normalsize

We can use the above regression to achieve the same:

$t-test =\widehat{\alpha}/se_{\widehat{\alpha}}$ \textcolor{student}{$=7.530/0.063=119.524$} 

\textcolor{student}{That $H_0$ makes no sense as smallest answer to A170 is 1.}


## Regression Analysis - Example 2

We now estimate a regression model which includes a constant and the household's monthly income (in 1,000 Euros) as an explanatory variable ($Inc_i$ or variable `X047D` in our dataset).

$LifeSatis_{i} = \alpha + \beta~ Inc_{i} + u_{i}$
\footnotesize
```{r, echo = TRUE, eval=TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE}
mod1 <- lm(A170~X047D,data=test_data)
```
\normalsize

How do we interprete the estimate of $\widehat{\beta}$?

## Regression Analysis - Example 2


\tiny
```{r, echo = TRUE, eval=TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE}
stargazer(mod1, type="text")
```
\normalsize
As the income increases by one unit (increase of Euro 1,000) we should expect that Life Satisfaction increases by 0.184 units.



## Regression Analysis - Example 2

\scriptsize
Let's present a graphical representation.

```{r, echo = TRUE, eval=TRUE, error = FALSE, message = FALSE, results = TRUE, warning = FALSE,fig.height = 2.0, fig.width = 4.5}
ggplot(test_data, aes(x=X047D, y=A170)) +
    labs(x = "Income", y = "Life Satisfaction") +
    geom_jitter(width=0.2, size = 0.5) +    # Use jitter - try geom_point() instead
    geom_abline(intercept = mod1$coefficients[1], 
                slope = mod1$coefficients[2], col = "blue")+
    ggtitle("Income v Life Satisfaction, Britain")
```
\normalsize


## Regression Analysis - What does it actually do?

Two interpretations

1) Finds the regression line (via $\widehat{\alpha}$ and $\widehat{\beta}$) that \textcolor{student}{~~minimises~~} the residual sum of squares $\Sigma (LifeSatis_{i} - \widehat{\alpha} - \widehat{\beta}~ Inc_{i})^2$.
$\rightarrow$ \textcolor{red}{Ordinary Least Squares (OLS)}
2) Finds the regression line (via $\widehat{\alpha}$ and $\widehat{\beta}$) that ensures that the residuals ($\widehat{u}_{i} = LifeSatis_{i} - \widehat{\alpha} - \widehat{\beta}~ Inc_{i}$) are \textcolor{student}{~~uncorrelated~~} with the explanatory variable(s) (here $Inc_{i}$).

In many ways 2) is the more insightful one. 

## Regression Analysis - What does it actually do?

$LifeSatis = \alpha + \beta~ Inc + u$

**Assumptions**

One of the regression assumptions is that the (unobserved) error terms $u$ are uncorrelated with the explanatory variable(s), here $Inc$. Then we call \textcolor{orange}{$Inc$ exogenous}.

This implies that $Cov(Inc,u)=Corr(Inc,u)=0$

**In sample**

$LifeSatis_{i} =$ \textcolor{blue}{$~~~\widehat{\alpha} + \widehat{\beta}~ Inc_{i}~~~$} $+ \widehat{u}$

Where \textcolor{blue}{$\widehat{\alpha} + \widehat{\beta}~ Inc_{i}$} is the regression-line. 

In sample $Corr(Inc_{i},\widehat{u}_{i})=0$ (is \textcolor{red}{ALWAYS TRUE BY CONSTRUCTION}).



## Regression Analysis - Underneath the hood?

$LifeSatis = \alpha + \beta~ Inc + u$ 

**What happens if you call** \\
`mod1 <- lm(A170~X047D,data=test_data)`?

You will recall the following from Year 1 stats:
\begin{eqnarray*}
\hat{\beta} &=& \dfrac{\widehat{Cov}(LifeSatis,Inc)}{\widehat{Var}(Inc)}\\
\hat{\alpha} &=& \overline{LifeSatis} - \hat{\beta} ~ \overline{Inc}
\end{eqnarray*}

The software will then replace $\widehat{Cov}(LifeSatis,Inc)$ and $\widehat{Var}(Inc)$ with their sample estimates to obtain $\hat{\beta}$ and then use that and the two sample means to get $\hat{\alpha}$.

## Regression Analysis - Underneath the hood?

Need to recognise that in a sample $\hat{\beta}$ and $\hat{\alpha}$ are really \textcolor{student}{random variables}.

\begin{eqnarray*}
\hat{\beta} &=& \dfrac{\widehat{Cov}(LifeSatis,Inc)}{\widehat{Var}(Inc)}\\
          &=&\dfrac{\widehat{Cov}(\alpha + \beta~ Inc + u,Inc)}{\widehat{Var}(Inc)}\\
          &=&\dfrac{\widehat{Cov}(\alpha,Inc) + \beta \widehat{Cov}(Inc,Inc) + \widehat{Cov}(u,Inc)}{\widehat{Var}(Inc)}\\
          &=& \beta ~\dfrac{\widehat{Var}(Inc)}{\widehat{Var}(Inc)}  + \dfrac{\widehat{Cov}(u,Inc)}{\widehat{Var}(Inc)}= \beta  + \dfrac{\widehat{Cov}(u,Inc)}{\widehat{Var}(Inc)}
\end{eqnarray*}

So $\hat{\beta}$ is a function of the random term $u$ and hence is itself a random variable.
Once $\widehat{Cov}(LifeSatis,Inc)$ and $\widehat{Var}(Inc)$ are replaced by sample estimates we get \textcolor{student}{~ONE~} value which is draw from a \textcolor{student}{random distribution.}

## Regression Analysis - The Exogeneity Assumption

Why is **assuming** $Cov(Inc,u)=0$ important when, in sample, we are guaranteed $Cov(Inc_{i},\widehat{u}_{i})=0$?

If $Cov(Inc_{i},u_{i})=0$ is **not true**, then

1) Estimating the model by OLS \textcolor{student}{imposes an incorrect relationship}
2) The estimated coefficients  \textcolor{blue}{$\widehat{\alpha}$ and $\widehat{\beta}$} are \textcolor{student}{biased (on average incorrect if we had many samples)}
3) The regression model has no \textcolor{student}{causal interpretation}

As we cannot observe $u_i$, the assumption of exogeneity cannot be tested and we need to make an argument using economic understanding.

## Regression Analysis - Outlook

$y = \alpha + \beta ~ x + u$

Much of empirical econometric analysis is about making the exogeneity assumption ($Corr(x,u)=0$) more plausible/as plausible as possible. But this begins with thinking why an explanatory variable $x$ is endogenous.

1) Most models have more than one explanatory variable.
2) Including more relevant explanatory variables can make the exogeneity assumption more plausible.
3) But fundamentally, if $Cov(u,x)=0$ is implausible we need to find another variable $z$ for which $Cov(u,z)=0$ is plausible. \textcolor{student}{A lot of the remainder of this unit is about elaborating on this issue.}


