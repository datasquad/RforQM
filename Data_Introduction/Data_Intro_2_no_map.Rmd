---
title: "Introduction to Data Handling and Statistics 2"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Preparing your workfile

We add the basic libraries needed for this week's work: 

```{r, message=FALSE, warning = FALSE}
library(tidyverse)    # for almost all data handling tasks
library(readxl)       # to import Excel data
library(ggplot2)      # to produce nice graphiscs
library(stargazer)    # to produce nice results tables
```


# Introduction

The example we are using here is taken from the CORE - Doing Economics resource. In particular we are using Project 8 which deals with international data on well-being. The data represent several waves of data from the European Value Study (EVS). A wave means that the same surevey is repeated at regular intervals (waves).


# Aim of this lesson

In this lesson we will revise some hypothesis testing and basic (simple) regression analysis. 

In terms of R skills you learn how to

* create summary tables using the `group_by`, `summaris` and `spread` commands
* create scatter plots using `ggplot` and `geom_point` 
* produce maps which colour coded to display summary statistics on a per country basis (using the `tmap` library)
* conduct simple hypothesis tests on one or two sample means using `t.test`
* use `for` loops in a little statistical experiment 
* run simple regression models using `lm`

# Importing Data

The data have been prepared as demonstrated in the Doing Economics Project 8, up to and including Walk-Through 8.3. Please have a look at this to understand the amount of data work required before an empirical analysis can begin. The datafile is saved as an R data structure (wb_data.Rdata).

```{r}
#wb_data <- readRDS("wellbeing_data.RDS")   # load RDS file
load("WBdata.Rdata")
str(wb_data)  # prints some basic info on variables
```

Checking your environment you will see two objects. Along the proper datafile (`wb_data`) you will find `wb_data_Des` which contains some information for each of the variables. It will help us to navigate the obscure variable names.

```{r}
wb_data_Des
```

As you can see there are a number of interesting questions in this dataset. These questions will allow us to investigate whether attitudes to work differ between coountries and whether such differences correlate to different levels of self-reported happiness/life satisfaction.

Details on the variables are avialable from [here](https://dbk.gesis.org/EVS/Variables/compview.asp) (chose EVS Longitudinal Data Files 1981-2008).

# Some initial data analysis and summary statistics

Let us investigate some of the features of this dataset. It has `r nrow(wb_data)` observations and `r length(wb_data)` variables. Let's see which countries are represented in our dataset.

```{r}
unique(wb_data$S003)   # unque finds all the different values in a variable
```

As you can see these are 48 countries, almost all European, Canada and the U.S. being the exceptions. In the same manner we can find out how many waves of data we have available.


```{r}
unique(wb_data$S002EVS)
```

Let's find out how many observations/respondents we have for each country in each year. To do this we will resort to the powerful piping technique delievered through the functionality of the `tidyverse`

```{r}
table1 <- wb_data %>% group_by(S002EVS,S003) %>% # groups by Wave and Country
            summarise(n = n()) %>%               # summarises each group by calculating obs
            spread(S002EVS,n) %>%                # put Waves across columns
            print(n=Inf)                         # n = Inf makes sure that all rows are printed

```

You can see that the number of countries have increased through time, although Canada and the U.S. have dropped out. 

If you look at the dataframe itself (either `view(wb_data)` or double click on the little spreadsheet icon on the right hand edge of the Environment window) you will recognise that there are a lot of missing observations (codes as `NA`). Ordinarily we would be interested in finding out how many effective observations we have for the life satisfaction variable (`A170`). However, the initial datawork in [https://www.core-econ.org/doing-economics/book/text/08-03.html#part-81-cleaning-and-summarizing-the-data](Project 8 of Doing Economics, Walk-Through 8.2), has made sure that all the observations you can see are those with available data for this variable.

Let's look at a couple of graphical representations of our data. For instance we may be interested in figuring out whether life satisfaction (1 (dissatisfied) to 10 (satisfied)) and self-reported health are related to each other. We shall look at this on the basis of data aggregated for country-waves.

```{r scatter_HvLS}
table2 <- wb_data %>% group_by(S002EVS,S003) %>% # groups by Wave and Country
            summarise(Avg_LifeSatis = mean(A170),Avg_Health = mean(A009))     # summarises each group by calculating obs
            
ggplot(table2,aes(Avg_Health,Avg_LifeSatis, colour=S002EVS)) +
  geom_point() +
  ggtitle("Health v Life Satisfaction")
```
We can see a clear positive relation between the two variables.

Let's see whether we can see a similarly clear relationship between life satisfaction and respondent's attitude towards work (`C041` - "Work should come first even if it means less spare time, 1 = Strongly Agree, 5 = Strongly Disagree"). What would you expect to see?

```{r scatter_WFvLS}
table2 <- wb_data %>% group_by(S002EVS,S003) %>% # groups by Wave and Country
            summarise(Avg_LifeSatis = mean(A170),Avg_WorkFirst = mean(C041))    # summarises each group by calculating obs

ggplot(table2,aes(Avg_WorkFirst, Avg_LifeSatis,colour=S002EVS)) +
  geom_point() +
  ggtitle("Work First v Life Satisfaction")
```

The relationship is less clear. Recall that small values for the "Work First" questions relate to the countries where, on average, respondents agreed more strongly with the statement!

```{r scatter_cLSWFvMedInc}
table3 <- wb_data %>% filter(S002EVS == "2008-2010") %>% 
            group_by(S003) %>% # groups by Country
            summarise(cor_LS_WF = cor(A170,C041,use = "pairwise.complete.obs"),
                      med_income = median(X047D)) %>%    # correlation, remove missing data
            arrange(cor_LS_WF) 

ggplot(table3,aes( cor_LS_WF, med_income)) +
  geom_point() +
  ggtitle("Corr(Life Satisfaction, Work First) v Median Income")

```

There isn't any really obvious relation between these correlations and the median income.

Let's create acouple of couloured maps which illustrate how median income and the above correlations are distributed across Europe.

# Hypothesis testing

Let's investigate whether there are differences in some of the responses between countries. But before we do so we need to revisit some basic hypothesis testing. At the core is the understanding that there is some underlying population statistic (for instance the difference between the average life satisfaction in the Great Britain and Germany), but all we observe is a sample statistics (the difference between the sample average of life satisfaction in the Great Britain and Germany). What hypothesis testing does is that it uses the sample information to help us judge on some hypothesis regarding the true underlying (unknown!) population statistic (for instance that the average life satisfaction in Germany and the U.K. are equal).

Let's create a sample statistic:

```{r}
test_data_G <- wb_data %>% 
  filter(S003 == "Germany") %>%     # pick German data
  filter(S002EVS == "2008-2010")    # pick latest wave

mean_G <- mean(test_data_G$A170)

test_data_GB <- wb_data %>% 
  filter(S003 == "Great Britain") %>%  # pick British data
  filter(S002EVS == "2008-2010")       # pick latest wave

mean_GB <- mean(test_data_GB$A170)

sample_diff <- mean_G - mean_GB
  
```

So we can see that the sample difference is `rsample_diff`, hence the average German response to the question on Life Satisfaction is 0.76 lower than that in Great Britain. There is the proof, Germans are just miserable. Or is it? If we had asked a different set of individuals we would have received a different statistic. Is this difference perhaps just the chance of some chance variation in the sample? It is to answer this question that we perform hypothesis tests.

In order to perform a hypothesis test we first formulate a null hypothesis. Here that the difference in population means (`mu`) is equal to 0 using the `t.test` function.

```{r}
t.test(test_data_G$A170,test_data_GB$A170, mu=0)  # testing that mu = 0
```

How can we use this information to evaluate our initial null hypothesis. To judge this we need to know what random distribution the sample test statistic (**if the null hypothesis was true**). In this case this is a normal distribution. The p-value then then tells us how likely it is to get a result like the one we got (a difference of -0.76 or larger) if the null hypothesis was true (i.e. the true population means were the same). Here the p-value is smaller than 2.2e-16, i.e. extremely close and hence we can say that the difference is extremely unlikely to be due to chance variation and indeed Germans are a miserable lot.

What about the difference between the Great Britain and Sweden though?

```{r}
test_data_SW <- wb_data %>% 
  filter(S003 == "Sweden") %>%  # pick British data
  filter(S002EVS == "2008-2010")       # pick latest wave

mean_SW <- mean(test_data_SW$A170)

t.test(test_data_SW$A170,test_data_GB$A170, mu=0)  # testing that mu = 0

```

Here you can see that the p-value is 0.1251, hence the probability of getting a result like the one we got, a difference in means of about 0.15, if the true population means were equal, is about 12.5%. Is that small enough for us to declare that we do not believe in the null hypothesis? This isn't so obvious any more. There are actually some "conventions" in the sense that we often say that we reject the null hypothes if that p-value is smaller than either 0.1, 0.05 or 0.01. 

But we shouldn't just adopt such a convention without understanding what these values mean. In order to do so we will add another variable to our dataset, a truly random variable, but drawn from the same distribution for all individuals

```{r}
wb_data$rvar <- rnorm(nrow(wb_data))   # add random variable
```

We will now check whether the average value for that variable differs between countries. Of course we know that it shouldn't as all observations are draws from the same random variable, a standard normal random variable and hence the true population mean for all countries **is 0**.

But let's pretend we didn't know that.

```{r}
test_data <- wb_data %>% 
  filter(S002EVS == "2008-2010")       # pick latest wave

countries <- unique(test_data$S003)  # List of all countries
n_countries <- length(countries)   # Number of countries, 46
```

Now we will perform the above test for all possible combinations of countries and will record the respective p-value. Don't worry too much about this double `for` loop. There is no need to understand the details of the code.

[Caution: The next bit of code can take quite a while on slower computers. If so, you can jump to the Regression Analysis part. Still try and understand what conceptually is being done here.]

```{r}
save_pvalue <- matrix(NA,n_countries,n_countries)

for (i in seq(2,n_countries)){
  for (j in seq(1,(i-1))){
    test_data_1 <- test_data %>% 
    filter(S003 == countries[i]) 
    mean_1 <- mean(test_data_1$A170)

    test_data_2 <- test_data %>% 
    filter(S003 == countries[j]) 
    mean_2 <- mean(test_data_2$A170)
    
    tt <- t.test(test_data_1$rvar,test_data_2$rvar, mu=0)  # testing that mu = 0
    save_pvalue[i,j] <- unlist(tt["p.value"])    # this will just pick the p-value
  }
}
```

This leaves us with (46*46-46)/2=1035 hypothesis tests. All of which of a null hypothesis which we know to be true (population means are identical). Let's see how many of these hypothesis tests delivered p-values which are smaller than 10%.

```{r HypTest1}
tre <- (save_pvalue<0.1)   # value of TRUE if pvalue < 0.1

cols <- c("TRUE" = "#FFFFFF","FALSE" = "#66FF33")

image(1:nrow(tre), 1:ncol(tre), as.matrix(tre), col=cols)

table(tre)
```

The green blots on the graph indicate rejections of the null hypothesis. As you can see, `r table(tre)[2]` of the 1035 tests produced a test statistic with a p-value smaller than 10%. So for these we may be tempted to say that we reject the null hypothesis. So here we have arrived at the point where we can perhaps understand what it means to perform a hypothesis test. Even if the null hypothesis is correct (which in reality we will of course not know) we may actually reject the null hypothesis. We call this making a Type 1 error. Vice versa, if in truth the null hypothesis is incorrect we may come to the conclusion not to reject the null hypothesis (this is what is called a Type 2 error).

As you can see here we have made a Type 1 error in about 10% of cases. This is no accident. If we had checked what percentage of these tests (remember for all the null hypothesis is true) had p-values < 5% we would have found approximately 5% of tests that had p-values smaller than 5%. In fact this is what a hypothesis test is designed to do. So this gives us now a clue of the role of this threshold against which we compare the p-value. 

You may wonder then why we do not use a threshold as small as possible, after all that would minimise the probability of making a Type 1 error. However, the flip side of reducing a Type 1 error is that we would at the same time increase the probability of making a Type 2 error, i.e. a failure to reject an incorrect null hypothesis.

Let's return to the Life Satisfaction data and repeat the above calculations.

```{r HypTest2}
save_pvalue <- matrix(NA,n_countries,n_countries)

for (i in seq(2,n_countries)){
  for (j in seq(1,(i-1))){
    test_data_1 <- test_data %>% 
    filter(S003 == countries[i]) 
    mean_1 <- mean(test_data_1$A170)

    test_data_2 <- test_data %>% 
    filter(S003 == countries[j]) 
    mean_2 <- mean(test_data_2$A170)
    
    tt <- t.test(test_data_1$A170,test_data_2$A170, mu=0)  # testing that mu = 0
    save_pvalue[i,j] <- unlist(tt["p.value"])    # this will just pick the p-value
  }
}

tre <- (save_pvalue<0.1)   # value of TRUE if pvalue < 0.1

cols <- c("TRUE" = "#FFFFFF","FALSE" = "#66FF33")

image(1:nrow(tre), 1:ncol(tre), as.matrix(tre), col=cols)

table(tre)
```

As is obvious from the visualisation we have many more rejections about 90%. For each combination of countries for which we reject the null hypothesis we say that there is a statistically significant difference in average life satisfaction. However, that does not mean that these differences represent meaningful differences. This is an important difference to keep in mind.

# Regression Analysis

Hypothesis testing is a crucial tool of empirical analysis. Another tool we will use repeatedly is that of regresison analysis. In fact, sometimes, running a regression is a convenient way to deliver a hyothesis test. Let us demonstrate this with one of the above examples, the difference in average life satisfaction between Great Britain and Sweden.

Let's start by creating a new dataset which only contains the British data. 

```{r}
test_data <- wb_data %>% 
  filter(S003 =="Great Britain") %>%  # pick British data
  filter(S002EVS == "2008-2010")         # pick latest wave
```

Now we run a regresison of the Life Satisfaction variable (`A170`) against a constant only.

$LifeSatis_{i} = \alpha + u_{i}$

```{r}
mod1 <- lm(A170~1,data=test_data)
stargazer(mod1, type="text")
```

What we will find is that the estimated value for $\alpha$, $\widehat{\alpha}=7.530$ is nothig else but the sample mean of all British observations in the 2008-2010 wave. We could now calculate a t-test $=\widehat{\alpha}/se{\widehat{\alpha}}$ $=7.530/0.063=119.524$ which tests the hypothesis that the average response to the Life Satisfaction question is equal to 0 (Yes, this makes no sense as the smallest possible response is 1, but R doesn't know that!). This could also be calculated

```{r}
t.test(test_data$A170, mu=0)  # testing that mu = 0
```

The differences are rounding differences.

Let's see how we could use a regression to test for the difference in means. First we adjust our dataset `test_data` to include British and Swedish data. Note her how we use the increadibly useful `S003 %in% c("Sweden","Great Britain")` condition which selects all observations for which the country variable (`S003`) is included in the list `c("Sweden","Great Britain")`. Type `?c` in the console to see what the `c90` function does.

```{r}
test_data <- wb_data %>% 
  filter(S003 %in% c("Sweden","Great Britain")) %>%  # pick British and Swedish data
  filter(S002EVS == "2008-2010")         # pick latest wave
```

Then we run a regression with the Life Expectancy as the dependent variable and a constant and a dummy variable which takes the value 1 if the respondent is from Sweden and 0 if the repondent is from Britain. This is achieved by specifying the model as `A170~S003`. The variable name before the `~` is the dependent variable, here `A170`. The variable after the `~` is the explanatory variable, here `S003`. (Note that R automatically includes a constant into the regression model, even if you do not specify it explicitely.)

```{r}
mod1 <- lm(A170~S003,data=test_data)
stargazer(mod1, type="text")
```

This regression is a very special one as it uses the country variable as the explanatory variable. Recall that `test_data` only contain British and Swedish data. The regression picked one country as the base country (here Britain as it is first in the alphabet) and for the other it created a dummy variable. Basically a variable ($Sweden_i$) which takes the value 1 if the observation comes from Sweden and 0 otherwise. 


$LifeSatis_{i} = \alpha + \beta~ Sweden_{i} + u_{i}$

You can see that the constant ($\widehat{\alpha}$) still reports the sample average for the British observations. It is identical to the value we saw in the previous regression. But what is the meaning of $\widehat{\beta}=0.149$? This is not the average response value for Swedish respondents, but the difference between the British and Swedish average. As it is positive it means that the Swedish average response is larger than the British. In fact it is 7.530+0.149=7.679. 

You can now test whether that difference is significantly different from 0 ($H_0:\beta = 0$) which is equivalent to testing that the two averages are identical. The t-test for this hypothesis test would be $0.149/0.097=1.536$ which, but for rounding differences, is identical to the test on equality of two means we performed previously.

The regressions we ran so far were special in the sense that they involved explanatory variables which were either a constant (i.e. ones) or dummy variables (0s or 1s). The result of this was that the resulting estimates represented sample means or differences in sample means.

The interpretation of coefficient estimates changes as explanatory variables take a more general form. 

```{r}
test_data <- wb_data %>% 
  filter(S003 =="Great Britain") %>%  # pick British data
  filter(S002EVS == "2008-2010")         # pick latest wave
```

We now estimate a regression model which includes a constant and the household's monthly income (in 1,000 Euros) as an explanatory variable ($INC_i$ OR VARIABLE `x047D` in our dataset).

$LifeSatis_{i} = \alpha + \beta~ Inc_{i} + u_{i}$

```{r}
mod1 <- lm(A170~X047D,data=test_data)
stargazer(mod1, type="text")
```

Here you see that $\widehat{\beta}=0.184$. What does this mean? As the income increases by one unit (here that represents an increase of Euro 1,000) we should expect that Life Satisfaction increases by 0.184 units. What is the interpretation for $\widehat{\alpha}=7.190$? For someone with 0 income we should expect the Life Satifaction to be 7.119. Let's present a graphical representation.

```{r RegPlot1}
ggplot(test_data, aes(x=X047D, y=A170, colour = S003)) +
    geom_jitter(width=0.2, size = 0.5) +    # Use jitter rather than point so we can see indiv obs
    geom_abline(intercept = mod1$coefficients[1], slope = mod1$coefficients[2])+
    ggtitle("Income v Life Satisfaction, Britain")
```

Note a few tweaks in the graphical representation. We use `geom_jitter` rather than `geom_point`. This adds some random noise to the data so that we can see the individual observation (replace `geom_jitter(width=0.2)` with `geom_point()` to see the difference it makes). `geom_abline` adds a line. We specify the intercept and slope from our regression model (`mod1$coefficients[1]` and `mod1$coefficients[2]`). `ggtitle` adds the title to the graph.

The regression parameters, which deliver the line of best fit, are estimated by Ordinary Least Squares (OLS). The name comes from the fact that these parameters are the ones which minimise the sum of squared residuals, $\Sigma \widehat{u}^2_i = \Sigma (LifeSatis_{i} - \widehat{\alpha} - \widehat{\beta}~ Inc_{i})^2$. These parameters achieve another thing, they ensure that $Corr(Inc_{i},\widehat{u}_{i})=0$ is true.

This last point is incredibly important, as one of the assumptions underpinning the estimation of regression models by OLS is that $Corr(Inc_{i},u_{i})=0$. Why is that assumption important? If the assumtion was not true, then we need to accept that the OLS estimation imposes a feature into the model that is not appropriate for the data. As a result the resulting regression coefficients are biased. As a consequence the resulting regression model cannot be said to have any causal interpretation. 

As we cannot observe $u_i$, the assumption of exogeneity cannot be tested directly and we need to make an argument using economic understanding.

A lot of econometric work is therefore directed at building either models or estimation methods (alternatives to OLS) which make this assumption more defendable. This could be the inclusion of additional explanatory variables (leading to multivariate regression analysis) or the application of alternative estimation methods (like instrumental variables estimation).
