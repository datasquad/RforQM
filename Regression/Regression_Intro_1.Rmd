---
title: "Introduction to Regression Analysis 1"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE)
```


# Preparing your workfile

We add the basic libraries needed for this week's work: 

```{r}
library(tidyverse)    # for almost all data handling tasks
library(readxl)       # to import Excel data
library(ggplot2)      # to produce nice graphiscs
library(stargazer)    # to produce nice results tables
library(haven)        # to import stata file
library(AER)          # access to HS robust standard errors
library(estimatr)     # use robust se
```


# Introduction

The data are an extract from the [Understanding Society Survey](https://www.understandingsociety.ac.uk/) (formerly the British Household Survey Panel).

# Data Upload - and understanding data structure

Upload the data, which are saved in a STATA datafile (extension `.dta`). There is a function which loads STATA file. It is called `read_dta` and is supplied by the `haven` package.

```{r}
data_USoc <- read_dta("20222_USoc_extract.dta")
data_USoc <- as.data.frame(data_USoc)    # ensure data frame structure
names(data_USoc)
```

Let us ensure that categorical variables are stored as `factor` variables. It is easiest to work with these in R.

```{r}
data_USoc$region <- as_factor(data_USoc$region)
data_USoc$male <- as_factor(data_USoc$male)
data_USoc$degree <- as_factor(data_USoc$degree)
data_USoc$race <- as_factor(data_USoc$race)
```

Click on the little table symbol in your environment tab to see the actual data table. 

The variable `pidp` contains a unique person identifier and the variable `wave` indicates the wave and `year` the year of observation.

To explain the meaning of these let us just pick out all the observations that pertain to one particular individual (`pidp == 272395767`). The following command does the following in words: "Take `data_USoc` filter/keep all observations which belong to individual pidp == 272395767, then select a list of variables (we don't need to see all 14 variables) and print the result":

```{r}
data_USoc %>% filter(pidp == 272395767) %>% 
              select(c("pidp","male","wave","year","paygu","age","educ")) %>% 
              print()
```

The same person (female) was observed three years in a row (from 2009 to 2011). Their gross monthly income changed, as did, of course, their age, but not their education. This particular person was observed in three consequitive waves. Let's se whether this is a commom pattern.

The code below figures out for how many individuals we have 1, 2 and 3 waves of observations. It is not important to understand that code.

```{r}
pattern <- data_USoc %>% group_by(pidp) %>% 
                      mutate(n_wave = n()) %>%  
                      select(pidp,n_wave) %>% 
                      unique() %>% 
                      group_by(n_wave)%>% 
                      summarise(n_pat = n()) %>% 
                      print()
```

As you can see only just over half of individuals have records for three waves. Let us look at the observations for an individual (`pidp == 2670365`) which only has observations for two waves.

```{r}
data_USoc %>% filter(pidp == 2670365) %>% 
              select(c("pidp","male","wave","year","paygu","age","educ")) %>% 
              print()
```

# Some summary statistics

Let's use the `stargazer` function to produce a nice summary table

```{r, message=FALSE, warning = FALSE}
stargazer(data_USoc, type = "text")
```


Here are some frequency tables

```{r}
data_USoc %>% count(wave) 
data_USoc %>% count(region) 
data_USoc %>% count(male) 
data_USoc %>% count(year) 
data_USoc %>% count(race) 
data_USoc %>% count(educ) 
data_USoc %>% count(degree) 
data_USoc %>% count(mfsize9) 
```

The pay information (`paygu`) is provided as a measure of the (usual) gross pay per month. We shall adjust for increasing price levels (as measured by `cpi`). We call this variable `hrpay` and also calculate the natural log of this variable (`lnhrpay`). Recall that, in order to change or create a new variable we use the `mutate` function.

```{r}
data_USoc <- data_USoc %>% 
              mutate(hrpay = paygu/(jbhrs*4)/(cpi/100)) %>% 
              mutate(lnhrpay = log(hrpay))
```

As we want to save these additional variables for further use we assign the result of the operation to `data_USoc`.

Let's check whether the log of the hourly pay differes if we analyse the data by certain criteria.

```{r}
data_USoc %>% group_by(degree) %>% 
              summarise(n = sum(!is.na(lnhrpay)), 
                        mean = mean(lnhrpay,na.rm=TRUE),
                        sd = sd(lnhrpay,na.rm=TRUE))

data_USoc %>% group_by(educ) %>% 
              summarise(n = sum(!is.na(lnhrpay)), 
                        mean = mean(lnhrpay,na.rm=TRUE),
                        sd = sd(lnhrpay,na.rm=TRUE))

data_USoc %>% group_by(male) %>% 
              summarise(n = sum(!is.na(lnhrpay)), 
                        mean = mean(lnhrpay,na.rm=TRUE),
                        sd = sd(lnhrpay,na.rm=TRUE))
```

You can see that difference in the averages for `lnhrpay` is about 0.183.

# Testing for differences

The first hypothesis test we may want to implement is to test whether the difference in the raw data is statistically significant (recall this is not the same economically significant difference).

We use the `t.test` function. We could create two subsets of data for `lnhrpay`, one for males and one for females and could then feed these two series into the `t.test` function (`t.test(data_male,data_female, mu = 0)`) but there is a more straightforward way to achieve this. We shall call `t.test(lnhrpay~male, mu=0, data = data_USoc)`. The `lnhrpay~male` is almost like a regression call, the variable we are interested in is `lnhrpay` but we want to know whether it differs according to `male`. The other inputs set the data frame (`data = data_USoc`) and the null hypothesis (`mu=0`).

```{r}
t.test(lnhrpay~male, mu=0, data = data_USoc)  # testing that mu = 0
```

So here the p-value is extremely small indicating that the raw differential in log hourly wages between males and females is certainly statistically significant.

# A regression - Gender differences

Regression analysis is our bread-and-butter technique and we can obtain the same result with a regression model.

```{r}
mod1 <- lm(lnhrpay~male,data = data_USoc)
cov1 <- vcovHC(mod1, type = "HC1")
robust_se <- sqrt(diag(cov1))
stargazer(mod1,type="text",se=list(NULL, robust_se))
```

In the regression output you can see the variable `male` but it says `malemale` which is indication that basically a dummy variable has been included into your regression that takes the value 1 for every male respondent and 0 for everyone else.

The coefficient you can see for that is 0.183 which is exactly the difference between the male and female averages for `lnhrpay`. 

If you were to merely calculate the regression model (`mod1 <- lm(lnhrpay~male,data = data_USoc)`) and then show the results with `stargazer`) you would get a very similar regression output, but the standard errors for the parameter estimates would have been calculated on the basis of a homoskedasticity assumption. Without any further details, this is an assumption which on many occasions is breached. However, the consequences are not too worrysome as long as we change how we calculate the standard errors. And that is what the extra lines achieve. 

We can actually write a little function which does add the HC robust standard errors to the usual output. It is not so important that you fully understand what is hapening here. Copy and paste the next code chunk into a new script file and save it as `stargazer_HC.r` into your working directory. 

```{r}
stargazer_HC <- function(mod, type_in = "text") {
  cov1 <- vcovHC(mod, type = "HC1")
  robust_se <- sqrt(diag(cov1))
  stargazer(mod,type=type_in,se=list(NULL, robust_se))
}
```

Once you have saved `stargazer_HC.r` into your working directory you need to make it accessible to your code by including

```{r}
source("stargazer_HC.r")
```

into your script. It is possibly best to do that right at the top where you load the packages (`library` commands).

Then we call a regresison and if we want robust standard errors we merely call `stargazer_HC` rather than `stargazer`). Added bonus is that you don't have to use the `type = "text"` option any longer as I used it as the default.

```{r}
mod1 <- lm(lnhrpay~male,data = data_USoc)
stargazer_HC(mod1)
```

Sometimes it is useful to extract the actual estimated coefficient and use it for some subsequent calculation. On this occasion let us extract the estimated cooefficient for `male`. When we estimated the regression model we saved the output in `mod1`. In fact `mod1` contains a lot of information we may want to use, most notably the coefficient estimates, estimated residuals, fitted values etc. We access these as follows

```{r}
mod1$coefficients
# mod1$residuals       # uncomment if you want to see these
# mod1$fitted.values
```
And if want a particular coefficient we can get that as follows

```{r}
mod1$coefficients["malemale"]    # or
# mod1$coefficients[1]
```

where we use the coefficient name we also saw in the regression output.

Let say we want to calculate the actual percentage points raw differential which is a function of this coefficient:

```{r}
(exp(mod1$coefficients["malemale"])-1)*100
```

Let's produce a scatter plot of the data on which the above regression is based.

```{r, warning=FALSE}
ggplot(data_USoc, aes(x = male, y = lnhrpay))  +
  geom_point() +
  geom_abline(intercept = mod1$coefficients[1], slope = mod1$coefficients[2])
```

As you can see this plot has limited informative value as, on the horizontal axis, we only have two possible outcomes, female and male. On each of these we have a wide range of outcomes for `lnhrpay`. We want to find a way to illustrate the distribution of outcomes on the `lnhrpay` scale depending on the value for `male`.

```{r, message=FALSE, warning=FALSE}
ggplot(subset(data_USoc, (lnhrpay>0) & (lnhrpay<5)), aes(x = lnhrpay))  +
  geom_histogram() +
  facet_grid(~male)

```

Recall that teh first input into the `ggplot` function is the dataframe we are using. HOwever, in the above command, instead of just using `data_USoc` we use `subset(data_USoc, (lnhrpay>0) & (lnhrpay<5))`. All that does is that we exclude some data, or better we select a subset of data, namely the data which have log hourly wage larger than 0 and smaller than 5. Try yourself how these histograms look if you do include all data (just ude `data_USoc`).

You could, if you wanted to, fit a normal distribution to these (remember the variables are the log hourly pay rates). This is not super straightforward and it turns out that in this case you would want to generate the graphs seperately. I had to google "R ggplot add normal distribution fit to geom_histogram" and came across [https://stackoverflow.com/questions/1376967/using-stat-function-and-facet-wrap-together-in-ggplot2-in-r/1379074#1379074](this) which helped me to adopt a solution. The normal density is added with a `stat_function`:

```{r, message=FALSE, warning=FALSE}
# first create a relevant subset of the data
data_temp <- data_USoc %>% 
            filter((lnhrpay>0) & (lnhrpay<5)) %>% # remove very small and large wages
            filter(male == "male") 

# now create the plot  
ggplot(data_temp,aes(x = lnhrpay))  +
      geom_histogram(binwidth = 0.25,aes(y = ..density..)) +   
      stat_function(fun = dnorm, args=list(mean=mean(data_temp$lnhrpay), 
                            sd=sd(data_temp$lnhrpay)), color = "darkred", size = 1) 

# Note: aes(y = ..density..) puts the histogram on a density scale
```

And now we can do the same for females.

```{r, message=FALSE, warning=FALSE}
# first create a relevant subset of the data
data_temp <- data_USoc %>% 
            filter((lnhrpay>0) & (lnhrpay<5)) %>% 
            filter(male == "female") 

# now create the plot  
ggplot(data_temp,aes(x = lnhrpay))  +
      geom_histogram(binwidth = 0.25,aes(y = ..density..)) +   
      stat_function(fun = dnorm, args=list(mean=mean(data_temp$lnhrpay), 
                            sd=sd(data_temp$lnhrpay)), color = "darkred", size = 1) 

```

# A regression - Education differences

After looking at gender differences we will now look at differences in earnings according to education differences.

Let's first look at the `degree` variable in our dataset.

```{r}
data_USoc %>% count(degree) 
```

Let us create a new variable which merely differentiates between having any degree or no degree. So we essentially want to collapse the `first degree` and `higher degree` categories into a `any degree` category. Recall that this is a factor variable and there is a very convenient function (`fct_recode`) which allows you to change the levels. The `mutate(grad = ...)` function creates a new variable with the definition of that variable following the equal sign.

```{r}
data_USoc <- data_USoc %>%
    mutate(grad = fct_recode(degree,
    "no degree"    = "no degree",    # new level = old level
    "any degree"  = "first degree",
    "any degree" = "higher degree"))
```

And now let's look at the counts of this new variable.

```{r}
data_USoc %>% count(grad) 
```

And now we run a regression just as we did for gender differences.

```{r}
mod1 <- lm(lnhrpay~grad,data = data_USoc)
stargazer_HC(mod1)
```

And let's calculate the actual percentage points raw differential which is a function of the estimated coefficient to the grad variable:

```{r}
(exp(mod1$coefficients["gradany degree"])-1)*100
```

Clearly there is a massive difference. Graduates, on average, earn more than 60% higher hourly wages.

# A regression - gender and education differences

We found that, individually, gender and degree status make significant differences to hourly pay. Let's use the full power of regression analysis and see how hourly pay changes as a function of both these factors.

```{r}
mod1 <- lm(lnhrpay~grad+male,data = data_USoc)
stargazer_HC(mod1)
```

Recall that both variables, `grad` and `male` have a base category (female and no degree respectively). It is best to think about this in form of a Table

| Name  | Gender | Degree | `male` | `grad` |
|------:|:-------|:-------|:-------|:-------|
| John | male | no | 1 | 0 |
| Maria | female | no | 0 | 0 |
| Jess | female | yes | 0 | 1 |
| Pete | male | yes | 1 | 1 |

The variable `male` will contribute to the conditional expectation for John and Pete and the `grad` variable will kick in for Jess and Pete. This sort of setup impies that the "effect"" of being male is the same regardless of whether you have a degree or not and also the "effect"" of having a degree is the same regardless of whether you are male or female. 

If you want a model setup that does not make that assumption you need to include an interaction term: 

```{r}
mod2 <- lm(lnhrpay~grad*male,data = data_USoc)
stargazer_HC(mod2)
```

The interaction term will take the value 1 only for those who are male and have a degree (in the above table only Pete). This setup allow for the "effect" of a degree to differ between males and females.