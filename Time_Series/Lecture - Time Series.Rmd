---
title: "Time-Series Modelling"
subtitle: "ECON20222 - Lecture 9"
author: "Ralf Becker and Martyn Andrews"
date: "March 2019"
output: 
  beamer_presentation:
    includes:
#      in_header: ../latex_template.tex
      in_header: ../latex_student.tex  # use for student version
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE)
```


## Aim for today

\begin{itemize}
  \item Understand the basic features of time-series data
  \item Understand autocorrelation
  \item Understand the difference between stationary and nonstationary data
  \item Understand how to build dynamic models that can be used for forecasting
\end{itemize}

## Purpose of time-series modelling (TS modelling)

There are typically three things econometricians want to achieve with time-series modelling

1. **Establishing causal relationships** between time-series.    
This is very difficult and in general causal relationships are more difficult to establis with TS modelling. It is not impossible but getting convincing exogenous variation is difficult.

2. Understanding the **dynamics in relationships between variables**.  
Questions like, "If the Central Bank changes the base rate, how long will it take for this to carry through to mortgage rates?" This is perfectly possible as long as we don't make strong causal statements (the CB may change base rates because mortgage rates are very low!!!)

3. **Forecasting one or several time series**.  
This is possibly the most common purpose of TS modelling. We will focus on this.

```{r, echo = FALSE}
library(tidyverse)
library(ggplot2)
library(pdfetch)
library(xts)
library(AER)          # access to HS robust standard errors
library(stargazer)
source("stargazer_HC.r")  # includes the robust regression display
source("stargazer_HAC.r")  # includes the Newey-West standard errors
library(gridExtra)  # required for the combination of ggplots
```

## Import some data into R

```{r, results = 'hide'}
rGDP <- pdfetch_ONS("ABMI","UKEA")  
periodicity(rGDP)   # check data availability
names(rGDP) <- "real GDP" # give a sensible name

# keep all the data including 2018-Q3
# this was the last observation available at the time this was written
# remove this line if you want to use updated data
rGDP <- rGDP["/2018-9"]  
```

`pdfetch` functions allow you to directly tap a umber of large data depositories:

* Bundesbank  
* Office for National Statistics (ONS)  
* Eurostats  
* FRED, etc  

```{r, echo = FALSE}
# we prepare the data for being kept in long format
# that is useful for plotting in ggplot
rGDP_l <- data.frame(index(rGDP),stack(as.data.frame(coredata(rGDP))))
# Give sensible names to columns
names(rGDP_l)[1] <- "Date"   # first col will have date
names(rGDP_l)[2] <- "Value"  # second col will have value
names(rGDP_l)[3] <- "id"     # third col will have series name
```


## An example

```{r, echo = FALSE}
ggplot(rGDP_l,aes(x =Date, y=Value)) + 
  geom_line(colour = "blue",size = 1.5) +
  ggtitle("UK real GDP") +
  theme_bw()
```

## The autocorrelation function (ACF)

Recall the correlation coefficient between two random variables $z_i$ and $p_i$ for two cross-sectional variables (index $i$)

\[Corr(z_i, p_i) = \frac{Cov(z_i, p_i)}{sd(z_i)sd(p_i)}\]

* a measure that expresses the strength of relationship between $z_i$ and $p_i$  
* it takes values in the interval $[-1,1]$  


## The autocorrelation function (ACF)

Consider the $y_t = rGDP_t$ time series

* we now use the subscript $t$  
* the subscript goes from $t = 1, ..., T$ where $T$ indicates how many observations we have
* here observations are quarterly (but other series can have other frequencies: e.g. annual, monthly, weekly, daily, hourly, etc. )
* here observations are from Q1 1955 to Q3 2018 (255 observations)

The ACF expresses how observations are correlated to observations 1, 2, 3 or $k$ observations prior.

How can you calculate a correlation of a series with itself?


## The autocorrelation function (ACF)

Let's consider the time series $y_t$ and the series one period prior, $y_{t-1}$. We also call $y_{t-1}$ a one period lag of $y_t$. 

\begin{tabular}{ccc}
	\hline 
	Observation & $y_t$ & $y_{t-1}$  \\ 
	\hline 
	
	0 & $y_{1955Q1}$ & NA \\ 
	
	1 & $y_{1955Q2}$ & $y_{1955Q1}$ \\ 
	 
	2 & $y_{1955Q3}$ & $y_{1955Q2}$  \\ 
	 
	3 & $y_{1955Q4}$ & $y_{1955Q3}$  \\ 

	\vdots & \vdots & \vdots \\
	
	253 & $y_{2018Q2}$ & $y_{2018Q1}$ \\

	254 & $y_{2018Q3}$ & $y_{2018Q2}$ \\	 
	\hline 
\end{tabular} 

Now we have "two" series for which we can calculate a correlation coefficient. We call this the first order autocorrelation coefficient $\rho_1$.

An ACF is a collection of autocorrelation coefficients calculated for longer lags $k$, $\rho_k$.


## The autocorrelation function (ACF)

In R this ACF is easily calculated using the `acf` function.

```{r, fig.height = 2.5, fig.width = 3.5}
temp_acf <- acf(rGDP)
```

## The autocorrelation function (ACF)

* The GDP series is strongly upward trending  
* this is comon for many macroeconomic series  
* this results in an ACF which has large $\rho_k$ for fairly large values of $k$ ($\rho_8 = 0.908$)
* we call this a persistant series

## Two further examples

```{r, echo = FALSE, fig.height = 2.5, fig.width = 4.5}
# Download: Female unemployment rate (YCPL in database LMS)
ur_female <- pdfetch_ONS("YCPL","LMS")
names(ur_female) <- "Unemp Rate (female)"

# keep all the data including 2018-Dec
# this was the last observation available at the time this was written
# remove this line if you want to use updated data
ur_female <- ur_female["/2018-12"]  

ur_female_l <- data.frame(index(ur_female),stack(as.data.frame(coredata(ur_female))))
names(ur_female_l)[1] <- "Date"
names(ur_female_l)[2] <- "Value"
names(ur_female_l)[3] <- "id"

# Download: Inflation rate (D7OE in database MM23)
infl <- pdfetch_ONS("D7OE","MM23")
names(infl) <- "CPI Inflation"

# keep all the data including 2019-Feb
# this was the last observation available at the time this was written
# remove this line if you want to use updated data
infl <- infl["/2019-2"]  

infl_l <- data.frame(index(infl),stack(as.data.frame(coredata(infl))))
names(infl_l)[1] <- "Date"
names(infl_l)[2] <- "Value"
names(infl_l)[3] <- "id"

data_l <- rbind(rGDP_l,ur_female_l)
data_l <- rbind(data_l,infl_l)

p1 <- ggplot(data_l[data_l$id == "Unemp Rate (female)",],aes(x =Date, y=Value)) + 
  geom_line(colour = "blue",size = 1.0) +
  ggtitle("Female Unemployment rate") +
  theme_bw()

p2 <- ggplot(data_l[data_l$id == "CPI Inflation",],aes(x =Date, y=Value)) + 
  geom_line(colour = "blue",size = 0.5) +
  ggtitle("Inflation Rate") +
  theme_bw()

grid.arrange(p1, p2, nrow=1, ncol=2)

```
These are data at a monthly frequency

## The ACF

```{r, echo = FALSE, fig.height = 3.5, fig.width = 4.5}
par(mfrow=c(1,2))

acf(ur_female,main = "Unemployment Rate")
acf(infl, main = "Inflation")
```

## The ACF

* the ACF shows that the unemployment rate is also very persistant and the ACF only slowly converges to 0.
$\Rightarrow$ a series can be oersitant without time trend

* The inflation rate is not persistant and the autocorrelation quickly drops towards 0. 
* But there are peaks of autocorrelation at frequencies 6 and 12 indicating some seasonal variation.

The ACF tells us something about how informative today's observation is for that in 1, 2, 3 or $k$ periods ahead. 

* If the ACF decays quickly to 0 then we will not be able to forecast a long period ahead.  
* If the ACF decays slowly then todays info is valuable for future observations


## Stationary and Nonstationary Series

The ACF expresses how persistant a series is.

* A series that is extremely persistant is called a **nonstationary** series.  
* A series that is not very persistant is called a **stationary** series.

Here: rGDP and unemployment rate are nonstationary. Inflation rate is stationary.

* In general series with a time-trend are nonstationary
* **BUT** there is a huge grey area inbetween.  

Formal statistical tests exist (e.g. Augmented Dickey-Fuller test) to decide (but they can be contradictory) and are not dealt with here. Here we eye-ball the series and look at how slowly the ACF converges to 0.

## Transformations

An important time-series transformation we consider is that of differencing a series.

\begin{tabular}{cccc}
	\hline 
	Observation & $y_t$ & $y_{t-1}$ & $\Delta y_t$ \\ 
	\hline 
	1 & $y_{1955Q2}$ & $y_{1955Q1}$ & $y_{1955Q2}-y_{1955Q1}= \Delta y_{1955Q2}$\\ 
	
	2 & $y_{1955Q3}$ & $y_{1955Q2}$ & $y_{1955Q3}-y_{1955Q2}= \Delta y_{1955Q3}$ \\ 
	
	\vdots & \vdots & \vdots & \vdots\\
	 
	\hline 
\end{tabular} 

Often we are actually much more interested in the difference of a series rather than the level. GDP is a case in point, the growth rate is what we are really interested in!

The GDP growth rate can be approximated for small growth rates by (assuming that $y_t$ is the GDP series)

\begin{eqnarray}
	gGDP_t &=& \frac{y_t - y_{t-1}}{y_{t-1}} \mathtt{~or}\\
	gGDP_t &=& ln(y_t) - ln(y_{t-1})
\end{eqnarray}

## ACF of differenced series

\begin{figure}[h]
	% \centering
	\includegraphics[width=0.8\textwidth]{r_GDP_UR.jpeg}
	\caption{TS and ACF plots for growth in GDP and unemployment rate, ONS}
	\label{Fig_g_GDP_UR}
\end{figure}

Differencing can turn nonstationary series into a stationary series.

## A simple regression

**Cross-Section Data**
\begin{equation}
y_t = \alpha + \beta x_t + u_t
\end{equation}

\begin{eqnarray}
E(u)&=&0\\
E(u|x)&=&0
\end{eqnarray}

this implied that $x_t$ was exogenous.

**Time-Series Data**

\begin{equation}
ur_t = \alpha + \beta ~ rGDP_t + u_t \label{Simple_TSR}
\end{equation}

## A simple regression

let's look at the end of the data table

\scriptsize

```{r, echo = FALSE}
ur_female_q <- to.period(ur_female,period="quarters")
ur_female_q <- ur_female_q$ur_female.Close
reg_data <- merge(rGDP, ur_female_q)
```
```{r}
# we multiply by 100 to express in percentage points, i.e. 0.5 is 0.5% or 0.005
reg_data$d_lgdp <- 100*diff(log(reg_data$real.GDP))
reg_data$d_lur <- 100*diff(log(reg_data$ur_female.Close)) 
tail(reg_data,10)

```

\normalsize

## A simple regression

\scriptsize
```{r}
mod1 <- lm(ur_female.Close~real.GDP,data = reg_data)
stargazer_HC(mod1)
```
\normalsize

## A simple regression - residual autocorrelation

\scriptsize
```{r, echo = FALSE, , fig.height = 3.5, fig.width = 4.5}
par(mfrow=c(1,2))
plot(mod1$residuals, type = "l", main = "mod1 - Residuals")
acf(mod1$residuals)
```
\normalsize

We can see that there is clear autocorrelation in the residuals. 

## A simple regression - testing for residual autocorrelation

We can again apply a hypothesis test the Breusch-Godfrey test (`bgtest`). The null hypothesis is that there is no autocorrelation.

```{r}
bgtest(mod1,order=4)
```

## A simple regression - HAC standard errors

When we estimate a regression which has autocorrelated error terms we need to apply a different formula to calculate standard errors for coefficients in a regression model (autoregressive heteroscedasticity consistent - HAC).

* They are called Newey-West standard errors.
* They are implemented in `stargazer_HAC.r`.  
* This will not change the coefficient estimates.   
* Will change the standard errors to the coefficients and hence inference (which will be incorrect if you don't use them).  
* If you have time-series data, in doubt, use Newey-West standard errors
* But crucial problems remain (see next slides)

In this example the standard errors only change marginally and hence are not shown here.
